{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install -U langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'api_key' #kicseréltem, hogy a githubon ne legyen benne a kulcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28175e-24b6-4939-8a3c-5a1f9511f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyGithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2502a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec0a82",
   "metadata": {},
   "source": [
    "## Github repo feldolgozása és indexelés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98eff590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from github import Github\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d88407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GitHub Repoból kiszedi a szükséges dolgokat (kód, dokumentáció)\n",
    "def clone_repo(github_token, repo_name):\n",
    "    g = Github(github_token)\n",
    "    repo = g.get_repo(repo_name)\n",
    "    \n",
    "    contents = repo.get_contents(\"\")\n",
    "    \n",
    "    files = []\n",
    "    while contents:\n",
    "        file_content = contents.pop(0)\n",
    "        if file_content.type == \"dir\":\n",
    "            contents.extend(repo.get_contents(file_content.path))\n",
    "        else:\n",
    "            if file_content.name.endswith(('.md', '.py', '.txt', '.json', '.yml', '.c', '.h', '.cpp', '.hpp')): # Ezeket a fájlokat fogadja el egyenlőre\n",
    "                file_data = {\n",
    "                    \"file_name\": file_content.name,\n",
    "                    \"file_content\": requests.get(file_content.download_url).text\n",
    "                }\n",
    "                files.append(file_data)\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d8dc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Issue-k kiszedése a repo-ból\n",
    "def fetch_issues(github_token, repo_name):\n",
    "    g = Github(github_token)\n",
    "    repo = g.get_repo(repo_name)\n",
    "    \n",
    "    issues = []\n",
    "    for issue in repo.get_issues(state='open'):\n",
    "        issue_data = {\n",
    "            \"file_name\": f\"issue_{issue.number}\",\n",
    "            \"file_content\": f\"Title: {issue.title}\\nDescription: {issue.body}\"\n",
    "        }\n",
    "        issues.append(issue_data)\n",
    "    \n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd639ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A fájlok kisebb darabokra bontása\n",
    "def preprocess_files_old(files):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "    documents = []\n",
    "    \n",
    "    for file in files:\n",
    "        content = file['file_content']\n",
    "        \n",
    "        #Markdown fájlok esetén HTML kód eltávolítása\n",
    "        if file['file_name'].endswith(('.md', '.html')):\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            content = soup.get_text()\n",
    "        \n",
    "        chunks = text_splitter.split_text(content)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            documents.append(chunk)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c0f266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "EXTENSION_LANGUAGE_MAP = {\n",
    "    '.py': Language.PYTHON,\n",
    "    '.cpp': Language.CPP,\n",
    "    '.c': Language.C,\n",
    "    '.cs': Language.CSHARP,\n",
    "    '.md': Language.MARKDOWN,\n",
    "    '.html': Language.HTML,\n",
    "}\n",
    "\n",
    "def get_language_from_filename(filename):\n",
    "    for ext, lang in EXTENSION_LANGUAGE_MAP.items():\n",
    "        if filename.endswith(ext):\n",
    "            return lang\n",
    "    return None\n",
    "\n",
    "def preprocess_files(files):\n",
    "    documents = []\n",
    "    \n",
    "    for file in files:\n",
    "        filename = file['file_name']\n",
    "        content = file['file_content']\n",
    "\n",
    "        if filename.endswith(('.md', '.html')):\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            content = soup.get_text()\n",
    "\n",
    "        lang = get_language_from_filename(filename)\n",
    "\n",
    "        if lang:\n",
    "            splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "                language=lang, chunk_size=500, chunk_overlap=0\n",
    "            )\n",
    "        else:\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "        docs = splitter.create_documents([content])\n",
    "        documents.extend(doc.page_content for doc in docs)\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3b3e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddingek generálása  OpenAI embeddings-el és FAISS indexeléssel\n",
    "def create_embeddings(documents):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    doc_embeddings = embeddings.embed_documents(documents)\n",
    "    doc_embeddings_np = np.array(doc_embeddings)\n",
    "    \n",
    "    index = faiss.IndexFlatL2(doc_embeddings_np.shape[1])\n",
    "    index.add(doc_embeddings_np)\n",
    "    \n",
    "    return index, doc_embeddings_np, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Egy konkrét repora alkalmazva a fenti funkciókat\n",
    "\n",
    "github_token = 'api_key'\n",
    "repo_name = \"pydantic/pydantic\"\n",
    "    \n",
    "files = clone_repo(github_token, repo_name)\n",
    "    \n",
    "issues = fetch_issues(github_token, repo_name)\n",
    "    \n",
    "all_documents = files + issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eec563b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joci\\AppData\\Local\\Temp\\ipykernel_10644\\3751607142.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "documents = preprocess_files(all_documents)\n",
    "    \n",
    "index, doc_embeddings_np, documents = create_embeddings(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8cf6e",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "354cf5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A kérdés (Query) feldolgozása és a legrelevánsabb dokumentumok visszaadása\n",
    "def retrieve_relevant_document(query, index, documents, embeddings, k=5):\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    query_embedding_np = np.array(query_embedding).reshape(1, -1)\n",
    "    D, I = index.search(query_embedding_np, k)\n",
    "    \n",
    "    relevant_docs = [documents[i] for i in I[0]]\n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7bd2000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 válasz legrevelánsabb dokumentum:\n",
      "- Title: Add support for Python 3.14\n",
      "Description: First 3.14 beta release is [planned on 2025-05-06](https://peps.python.org/pep-0745/#release-schedule) and PEP 649/749 is almost fully implemented.\n",
      "\n",
      "Considering the significant changes it provides to the runtime evaluation of type hints, we should add support to 3.14 and report any bugs/issues.\n",
      "- python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
      "                     platform: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "             related packages: mypy-1.8.0 typing_extensions-4.9.0\n",
      "                       commit: unknown\n",
      "```\n",
      "- python version: 3.12.7 (main, Oct  1 2024, 11:15:50) [GCC 14.2.1 20240910]\n",
      "                     platform: Linux-6.11.5-arch1-1-x86_64-with-glibc2.40\n",
      "             related packages: typing_extensions-4.12.2 fastapi-0.115.0 mypy-1.13.0 pydantic-settings-2.6.1\n",
      "                       commit: unknown\n",
      "```\n",
      "- python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n",
      "                     platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\n",
      "             related packages: typing_extensions-4.10.0 mypy-1.9.0 pydantic-settings-2.2.1\n",
      "                       commit: unknown\n",
      "```\n",
      "- python version: 3.11.8 (main, Feb 25 2024, 16:41:26) [GCC 9.4.0]\n",
      "                     platform: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\n",
      "             related packages: typing_extensions-4.10.0\n",
      "                       commit: unknown\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "#Példa kérdés\n",
    "query = \"Does this program support Python 3.14?\"\n",
    "relevant_docs = retrieve_relevant_document(query, index, documents, OpenAIEmbeddings())\n",
    "    \n",
    "print(f\"5 válasz legrevelánsabb dokumentum:\")\n",
    "for doc in relevant_docs:\n",
    "        print(f\"- {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4461264-5cac-479a-917c-9bf589826da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the context provided, the program does not currently support Python 3.14.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 533, 'total_tokens': 552, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BM2DKRgAVmnj4ngloqv1BNzqW7DX2', 'finish_reason': 'stop', 'logprobs': None}, id='run-dca61ff7-0238-41a0-89ec-73154f63f3cf-0', usage_metadata={'input_tokens': 533, 'output_tokens': 19, 'total_tokens': 552, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\":relevant_docs,\"question\":\"Does this program support Python 3.14?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb37059",
   "metadata": {},
   "source": [
    "## Kiértékelés RAGAs-sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02eab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4438164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joci\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [\"Does this program support Python 3.14?\", \n",
    "             \"What is the purpose of the REDIRECT_TO_V1 dictionary?\",\n",
    "             \"What is the purpose of the is_root_model function\",\n",
    "            ]\n",
    "ground_truths = [\"No, this program supports Python 3.14. It is only planned for the future.\",\n",
    "                \"The REDIRECT_TO_V1 dictionary automatically redirects certain utility functions from the main Pydantic namespace to their v1 implementations, generating a warning message that informs users about the change while maintaining backward compatibility. This helps manage the transition between major versions by ensuring old code continues to work while encouraging updates to newer patterns.\",\n",
    "                \"The is_root_model function determines whether a given TypeInfo represents a RootModel subclass or the RootModel class itself. It does this by checking if the type has a base class matching the ROOT_MODEL_FULLNAME constant, which is defined as 'pydantic.root_model.RootModel'.\"]\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for query in questions:\n",
    "  context = retrieve_relevant_document(query, index, documents, OpenAIEmbeddings())\n",
    "  answers.append(chain.invoke({\"context\": context,\"question\":query}).content)\n",
    "  contexts.append(context)\n",
    "\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"reference\": ground_truths\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19e3ace3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['Does this program support Python 3.14?',\n",
       "  'What is the purpose of the REDIRECT_TO_V1 dictionary?',\n",
       "  'What is the purpose of the is_root_model function'],\n",
       " 'answer': ['Based on the provided context, the program does not currently support Python 3.14.',\n",
       "  \"The purpose of the REDIRECT_TO_V1 dictionary is to redirect certain objects from the 'pydantic.utils' module to the 'pydantic.v1.utils' module.\",\n",
       "  \"The purpose of the `is_root_model` function is to convert the `RootModel` to a dictionary with the key `'root'`.\"],\n",
       " 'contexts': [['Title: Add support for Python 3.14\\nDescription: First 3.14 beta release is [planned on 2025-05-06](https://peps.python.org/pep-0745/#release-schedule) and PEP 649/749 is almost fully implemented.\\n\\nConsidering the significant changes it provides to the runtime evaluation of type hints, we should add support to 3.14 and report any bugs/issues.',\n",
       "   'python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\\r\\n                     platform: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\\r\\n             related packages: mypy-1.8.0 typing_extensions-4.9.0\\r\\n                       commit: unknown\\r\\n```',\n",
       "   'python version: 3.12.7 (main, Oct  1 2024, 11:15:50) [GCC 14.2.1 20240910]\\r\\n                     platform: Linux-6.11.5-arch1-1-x86_64-with-glibc2.40\\r\\n             related packages: typing_extensions-4.12.2 fastapi-0.115.0 mypy-1.13.0 pydantic-settings-2.6.1\\r\\n                       commit: unknown\\r\\n```',\n",
       "   'python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\\r\\n                     platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\\r\\n             related packages: typing_extensions-4.10.0 mypy-1.9.0 pydantic-settings-2.2.1\\r\\n                       commit: unknown\\r\\n```',\n",
       "   'python version: 3.11.8 (main, Feb 25 2024, 16:41:26) [GCC 9.4.0]\\r\\n                     platform: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\\r\\n             related packages: typing_extensions-4.10.0\\r\\n                       commit: unknown\\n```'],\n",
       "  [\"REDIRECT_TO_V1 = {\\n    f'pydantic.utils:{obj}': f'pydantic.v1.utils:{obj}'\\n    for obj in (\\n        'deep_update',\\n        'GetterDict',\\n        'lenient_issubclass',\\n        'lenient_isinstance',\\n        'is_valid_field',\\n        'update_not_none',\\n        'import_string',\\n        'Representation',\\n        'ROOT_KEY',\\n        'smart_deepcopy',\\n        'sequence_like',\\n    )\\n}\",\n",
       "   '* ♻️ Use different error message on v1 redirects by @Kludex in [#6595](https://github.com/pydantic/pydantic/pull/6595)\\n* ⬆ Upgrade `pydantic-core` to v2.2.0 by @lig in [#6589](https://github.com/pydantic/pydantic/pull/6589)\\n* Fix serialization for IPvAny by @dmontagu in [#6572](https://github.com/pydantic/pydantic/pull/6572)\\n* Improve CI by using PDM instead of pip to install typing-extensions by @adriangb in [#6602](https://github.com/pydantic/pydantic/pull/6602)',\n",
       "   'Having this work consistently would be nice for defining dictionaries with certain required keys, but allowing for arbitrary extras.',\n",
       "   '## v1.1 (2019-11-07)',\n",
       "   \"- It's a change in comparison with `V1`\\r\\n- We need users' feedback/requests on it to decide about the behavior\\r\\n\\r\\nSo, there are two possible options:\\r\\n\\r\\n- we just allow this to fine, or\\r\\n- we add a switch to prevent access (`__getattr__` and `__setattr__`) on extra attributes which match a protected namespace\"],\n",
       "  ['::: pydantic.root_model',\n",
       "   '```\\n\\nThis trick is actually used in [`RootModel`](models.md#rootmodel-and-custom-root-types) for precisely this purpose.',\n",
       "   'Note that for [root models](#rootmodel-and-custom-root-types), the root value can be passed to\\n[`model_construct()`][pydantic.main.BaseModel.model_construct] positionally, instead of using a keyword argument.\\n\\nHere are some additional notes on the behavior of [`model_construct()`][pydantic.main.BaseModel.model_construct]:',\n",
       "   \"```\\n\\nNote also that [`RootModel`](models.md#rootmodel-and-custom-root-types) _does_ get converted to a dictionary with the key `'root'`.\",\n",
       "   \"def test_root_model_nested():\\n    calls = []\\n\\n    class B(RootModel[int]):\\n        def my_b_method(self):\\n            calls.append(('my_b_method', self.root))\\n\\n    class A(RootModel[B]):\\n        def my_a_method(self):\\n            calls.append(('my_a_method', self.root.root))\\n\\n    m1 = A.model_validate(1)\\n    m1.my_a_method()\\n    m1.root.my_b_method()\\n    assert calls == [('my_a_method', 1), ('my_b_method', 1)]\"]],\n",
       " 'reference': ['No, this program supports Python 3.14. It is only planned for the future.',\n",
       "  'The REDIRECT_TO_V1 dictionary automatically redirects certain utility functions from the main Pydantic namespace to their v1 implementations, generating a warning message that informs users about the change while maintaining backward compatibility. This helps manage the transition between major versions by ensuring old code continues to work while encouraging updates to newer patterns.',\n",
       "  \"The is_root_model function determines whether a given TypeInfo represents a RootModel subclass or the RootModel class itself. It does this by checking if the type has a base class matching the ROOT_MODEL_FULLNAME constant, which is defined as 'pydantic.root_model.RootModel'.\"]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe618392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 12/12 [00:10<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a1ccd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does this program support Python 3.14?</td>\n",
       "      <td>[Title: Add support for Python 3.14\\nDescripti...</td>\n",
       "      <td>Based on the provided context, the program doe...</td>\n",
       "      <td>No, this program supports Python 3.14. It is o...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.989978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of the REDIRECT_TO_V1 dict...</td>\n",
       "      <td>[REDIRECT_TO_V1 = {\\n    f'pydantic.utils:{obj...</td>\n",
       "      <td>The purpose of the REDIRECT_TO_V1 dictionary i...</td>\n",
       "      <td>The REDIRECT_TO_V1 dictionary automatically re...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of the is_root_model function</td>\n",
       "      <td>[::: pydantic.root_model, ```\\n\\nThis trick is...</td>\n",
       "      <td>The purpose of the `is_root_model` function is...</td>\n",
       "      <td>The is_root_model function determines whether ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.963927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0             Does this program support Python 3.14?   \n",
       "1  What is the purpose of the REDIRECT_TO_V1 dict...   \n",
       "2  What is the purpose of the is_root_model function   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Title: Add support for Python 3.14\\nDescripti...   \n",
       "1  [REDIRECT_TO_V1 = {\\n    f'pydantic.utils:{obj...   \n",
       "2  [::: pydantic.root_model, ```\\n\\nThis trick is...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Based on the provided context, the program doe...   \n",
       "1  The purpose of the REDIRECT_TO_V1 dictionary i...   \n",
       "2  The purpose of the `is_root_model` function is...   \n",
       "\n",
       "                                           reference  context_precision  \\\n",
       "0  No, this program supports Python 3.14. It is o...                1.0   \n",
       "1  The REDIRECT_TO_V1 dictionary automatically re...                1.0   \n",
       "2  The is_root_model function determines whether ...                0.0   \n",
       "\n",
       "   context_recall  faithfulness  answer_relevancy  \n",
       "0             0.5           0.5          0.989978  \n",
       "1             0.0           1.0          1.000000  \n",
       "2             0.0           0.5          0.963927  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b341c4",
   "metadata": {},
   "source": [
    "## RAFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a854cd8",
   "metadata": {},
   "source": [
    "## Adathalmaz készítése a RAFT-hoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c11f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_qa_data = {\n",
    "    \"pydantic/pydantic\": [\n",
    "        {\"question\": \"Does this program support Python 3.14?\", \"answer\": \"No, this program supports Python 3.14. It is only planned for the future.\"},\n",
    "        {\"question\": \"What is the purpose of the REDIRECT_TO_V1 dictionary?\", \"answer\": \"The REDIRECT_TO_V1 dictionary automatically redirects certain utility functions from the main Pydantic namespace to their v1 implementations, generating a warning message that informs users about the change while maintaining backward compatibility. This helps manage the transition between major versions by ensuring old code continues to work while encouraging updates to newer patterns.\"},\n",
    "        {\"question\": \"What is the purpose of the is_root_model function?\", \"answer\": \"The is_root_model function determines whether a given TypeInfo represents a RootModel subclass or the RootModel class itself. It does this by checking if the type has a base class matching the ROOT_MODEL_FULLNAME constant, which is defined as 'pydantic.root_model.RootModel'.\"},\n",
    "        {\"question\": \"What exactly does that refer to that we want reduce the dependence on callables to mutate JSON schemas. What exactly does this refer to, and why is it desirable based on the information provided?\", \"answer\": \"The desire to reduce this dependence stems from the description of the existing callable annotation construct as nebulous and the stated goal to perform simple dict updates when possible instead, suggesting the current callable-based approach is considered complex or unclear compared to more direct methods\"},\n",
    "        {\"question\": \"The definition for JsonValue uses a complex structure involving Union, Annotated, Tag, and a callable Discriminator (_get_type_name). How does this combination specifically ensure that only JSON-compatible types (like lists, dicts with string keys, strings, numbers, booleans, None) and nested structures of these types are validated?\", \"answer\": \"JsonValue uses a Union of basic JSON-compatible types (list, dict, str, int, float, bool, None), where each type is Annotated with a Tag (its type name). A Discriminator function (_get_type_name) identifies the input's type name. Pydantic matches this name to the Tag to select the correct validator. The list and dict types are defined recursively (list['JsonValue'], dict[str, 'JsonValue']) to handle nested structures. If the input type doesn't match any tag, validation fails. An internal annotation (_AllowAnyJson) also handles parsing if the input is a JSON string.\"},\n",
    "    ],\n",
    "    \"pallets/flask\": [\n",
    "        {\"question\": \"What is Flask\", \"answer\": \"Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. It began as a simple wrapper around Werkzeug and Jinja, and has become one of the most popular Python web application frameworks.\"},\n",
    "        {\"question\": \"The PathDispatcher example uses the shift_path_info function from wsgiref.util after identifying the target application but before calling it. What specific function does shift_path_info perform in this context?\", \"answer\": \"The document shows the PathDispatcher calling shift_path_info(environ) after retrieving the application (app) based on the path prefix and before calling app(environ, start_response). However, the document does not explicitly explain what the shift_path_info function does or why it is necessary in this specific dispatching scenario. It only demonstrates its usage within the code example.\"},\n",
    "        {\"question\": \"Is there a security vulnerability for Jinja2 GHSA-cpwx-vrp4-4pq7?\", \"answer\": \"No, the issue was closed\"},\n",
    "        {\"question\": \"The methods do_teardown_request and do_teardown_appcontext define their exc parameter with a default value of _sentinel (imported from .sansio.scaffold). Why does the code use this specific sentinel object as the default instead of None?\", \"answer\": \"he code shows that if the exc parameter is the _sentinel object upon entering the do_teardown_request or do_teardown_appcontext methods, it retrieves the current exception information using sys.exc_info()[1]. This allows the functions to detect if an exception occurred during the request/app context lifecycle even if no exception was explicitly passed to the teardown function. However, the code itself does not explicitly explain the rationale for choosing the _sentinel object over using None as the default to distinguish between no exception passed and potentially None passed explicitly.\"},\n",
    "        {\"question\": \"The flash function contains a comment explaining that the original implementation using session.setdefault('_flashes', []).append(...) had issues. what was this issue, and how does the current implementation address it?\", \"answer\": \"According to the comment within the flash function, the original implementation assumed that modifying the list obtained via session.setdefault() would always update the session object itself. This assumption is incorrect for session implementations that use external storage (where the session object and its stored data might not be the same in-memory object). The current implementation addresses this by explicitly retrieving the list using session.get(_flashes, []), appending the new message to this retrieved list, and then explicitly assigning the modified list back to the session via session[_flashes] = flashes, ensuring the change is correctly persisted regardless of the session backend.\"},\n",
    "    ],\n",
    "    \"gandalfcode/gandalf\": [\n",
    "        {\"question\": \"The RiemannSolver constructor accepts a zeroMassFlux boolean parameter, which is later used in the ExactRiemannSolver::ComputeFluxes method. According to the comments and code within this file, what is the specific purpose of this zeroMassFlux option, and how does the code modify the flux calculation when it is enabled?\", \"answer\": \"According to comments in ComputeFluxes, the zeroMassFlux boolean option is intended for use with the Meshless finite-mass scheme of Hopkins 2015, specifically when the computational face moves with the velocity of the star region (ustar). When this option is true, the code implements the zero mass flux condition by:Setting the normal velocity component (Wface[ivx]) of the state sampled at the interface (s=0) to zero. Modifying the input face velocity (vface) by adding the star region's velocity (u, which is ustar at s=0) projected onto the original coordinate axes (u*runit[k]). The file does not provide further details on the Hopkins scheme itself or the theoretical justification for these specific modifications beyond stating they achieve zero mass flux in that context.\"},\n",
    "        {\"question\": \"What is the purpose of the gamma parameter in the RiemannSolver?\", \"answer\": \"Adiabatic index\"},\n",
    "        {\"question\": \"Based on the parameters read from the configuration file, how does the ProcessSphParameters function determine which specific SPH kernel class (like M4Kernel, QuinticKernel, GaussianKernel, or TabulatedKernel) should be instantiated for the sph object?\", \"answer\": \"The function selects the SPH kernel class by first checking the tabulated_kernel integer parameter, and if it's zero, it then uses the kernel string parameter to choose between M4Kernel, QuinticKernel, or GaussianKernel.\"},\n",
    "        {\"question\": \"The PlotCommand.processCommand method includes a section described as a Hack for deleting the old colorbar when overplot is false. According to the code and comments, why is this necessary, and what specific steps does it take to remove the old colorbar?\", \"answer\": \"According to the code and comments, this hack is performed when overplot is false to remove any existing colorbar associated with the axes (ax) before drawing a new plot. It works by retrieving the colorbar object (if it exists) from an internal plotting.axesimages dictionary, explicitly deleting the colorbar's axes from the figure (fig.delaxes(cbar.ax)), and then adjusting the geometry of the main plot's axes (ax.change_geometry(...)) presumably to reclaim the space. The code doesn't explicitly state why ax.clear() alone is insufficient for this.\"},\n",
    "        {\"question\": \"In the neighbour finding section, why does the code calculate the angle between the source-particle vector and the source-neighbour vector instead of just selecting the closest neighbour that is also closer to the source?\", \"answer\": \"The code calculates the angle to find the neighbour that lies closest to the direct line-of-sight path between the particle and the source, rather than just the nearest particle, to build a chain for subsequent photon absorption calculations along that path.\"},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bee70cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_everything(repo_name, github_token):\n",
    "    files = clone_repo(github_token, repo_name)\n",
    "    issues = fetch_issues(github_token, repo_name)\n",
    "    all_documents = files + issues\n",
    "    documents = preprocess_files(all_documents)\n",
    "    index, doc_embeddings_np, documents = create_embeddings(documents)\n",
    "    return index, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "281409ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_data_with_index_documents = []\n",
    "\n",
    "for repo_name, qa_pairs in repo_qa_data.items():\n",
    "    index, documents = generate_everything(repo_name, github_token)\n",
    "    for qa in qa_pairs:\n",
    "        repo_data_with_index_documents .append({\n",
    "            \"question\": qa[\"question\"],\n",
    "            \"answer\": qa[\"answer\"],\n",
    "            \"index\": index,\n",
    "            \"documents\": documents\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5fb19c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train és test adatok szétválasztása\n",
    "import random\n",
    "random.seed(12) \n",
    "shuffled = repo_data_with_index_documents[:]\n",
    "random.shuffle(shuffled)\n",
    "split_point = int(len(repo_data_with_index_documents) * 0.9)\n",
    "repo_data_train = shuffled[:split_point]\n",
    "repo_data_test = shuffled[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65dd2a2",
   "metadata": {},
   "source": [
    "## Adathalmaz átalakítása a fine tuning-hoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "616b0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DISTRACTORS = 4 #Distractor dokumentumok száma\n",
    "PERCENT_D_STAR_INCLUDED = 0.8 #D* arány"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4449edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A D* megtalálásához szükséges segéd függvény\n",
    "#Egy llm-et kérdez meg, hogy a kérdés válasz alapján melyik dokumentumban van a válasz\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.2)\n",
    "\n",
    "def call_llm_to_find_doc(question, answer, candidate_docs):\n",
    "    prompt = f\"\"\"You are an AI assistant helping with document understanding.\n",
    "\n",
    "Question: \"{question}\"\n",
    "Answer: \"{answer}\"\n",
    "\n",
    "Here are some candidate document chunks (labeled D1, D2, ..., Dn):\n",
    "\n",
    "\"\"\"\n",
    "    for i, doc in enumerate(candidate_docs):\n",
    "        prompt += f\"D{i+1}: {doc}\\n\\n\"\n",
    "\n",
    "    prompt += \"\"\"\n",
    "Please analyze the documents above and decide which document (D1, D2, ..., Dn) most likely contains the answer to the question. \n",
    "\n",
    "If none of the documents are relevant or contain the answer, reply with \"None\".\n",
    "Otherwise, reply with the label of the best matching document, such as \"D2\".\n",
    "Only respond with \"None\" or a label like \"D3\"—no explanation.\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    reply = response.content.strip()\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A D* megtalálásához szükséges fő függvény\n",
    "#Megkeresi a dokumentumot a kérdés-válasz alapján, amiben a válasz található\n",
    "def find_D_star_document(question, answer, all_docs, embeddings, index):\n",
    "    candidate_docs = retrieve_relevant_document(question, index, all_docs, embeddings, k=10)\n",
    "\n",
    "    if not candidate_docs:\n",
    "        print(\"Nem találtunk releváns dokumentumot.\")\n",
    "        return None\n",
    "\n",
    "    best_doc_label = call_llm_to_find_doc(question, answer, candidate_docs)\n",
    "\n",
    "    best_doc_label = best_doc_label.strip('\"')\n",
    "    if best_doc_label.lower() == \"none\":\n",
    "        return None\n",
    "\n",
    "    if best_doc_label.startswith(\"D\") and best_doc_label[1:].isdigit():\n",
    "        doc_index = int(best_doc_label[1:]) - 1\n",
    "        if 0 <= doc_index < len(candidate_docs):\n",
    "            return candidate_docs[doc_index]\n",
    "\n",
    "    print(\"A LLM válasza nem értelmezhető:\", best_doc_label)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ddafb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_distractor_documents(question, D_star_doc, all_docs, embeddings, index, k):\n",
    "    retrieved_docs = retrieve_relevant_document(question, index, all_docs, embeddings, k=k*2 + 1) #Több választ kér, hogy legyen benne Di is\n",
    "\n",
    "    distractors = []\n",
    "    for doc in retrieved_docs:\n",
    "        if doc != D_star_doc and doc not in distractors: #Ellenőrzi, hogy ne legyen benne a D* dokumentum\n",
    "            distractors.append(doc)\n",
    "        if len(distractors) == k:\n",
    "            break\n",
    "\n",
    "    #Ha nincs még elég distractor, akkor random választ a többi dokumentumból\n",
    "    attempts = 0\n",
    "    max_attempts = k * 5\n",
    "    while len(distractors) < k and attempts < max_attempts:\n",
    "        random_doc = random.choice(all_docs)\n",
    "        if random_doc != D_star_doc and random_doc not in distractors:\n",
    "            distractors.append(random_doc)\n",
    "        attempts += 1\n",
    "\n",
    "    return distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e894effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chain-of-Thought (CoT) generálás\n",
    "def generate_cot_answer(question, D_star_doc, original_answer):\n",
    "    prompt = f\"\"\"Given the Question, the Context (containing the golden document with the answer), and the original concise Answer, provide a detailed reasoning process (Chain-of-Thought) that explains step-by-step how to arrive at the Answer using *only* the provided Context.\n",
    "\n",
    "Crucially, you MUST cite the exact sentences or phrases from the Context that support your reasoning. Enclose these citations within ##begin_quote## and ##end_quote## tags. Do not add any information not present in the Context.\n",
    "\n",
    "Finally, state the concise Answer clearly. Format your response *exactly* as:\n",
    "##Reason: [Your detailed reasoning with citations like ##begin_quote## text from context ##end_quote##.]\n",
    "##Answer: [The final concise answer, matching the original answer provided]\n",
    "\n",
    "Question: {question}\n",
    "Context: {D_star_doc}\n",
    "Original Answer: {original_answer}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    reply = response.content.strip()\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c9d85",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m all_document_chunks \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Először meg kell találni a D* dokumentumot\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m D_star_doc \u001b[38;5;241m=\u001b[39m \u001b[43mfind_D_star_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_document_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m D_star_doc:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m kérdés kihagyása. D* dokumentum nem található.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[116], line 4\u001b[0m, in \u001b[0;36mfind_D_star_document\u001b[1;34m(question, answer, all_docs, embeddings, index)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_D_star_document\u001b[39m(question, answer, all_docs, embeddings, index):\n\u001b[1;32m----> 4\u001b[0m     candidate_docs \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_relevant_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidate_docs:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNem találtunk releváns dokumentumot.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m, in \u001b[0;36mretrieve_relevant_document\u001b[1;34m(query, index, documents, embeddings, k)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_relevant_document\u001b[39m(query, index, documents, embeddings, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     query_embedding_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(query_embedding)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m     D, I \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(query_embedding_np, k)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_community\\embeddings\\openai.py:700\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;124;03m        Embedding for the text.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_community\\embeddings\\openai.py:671\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_community\\embeddings\\openai.py:497\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    495\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 497\u001b[0m     response \u001b[38;5;241m=\u001b[39m embed_with_retry(\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size],\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params,\n\u001b[0;32m    501\u001b[0m     )\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    503\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_community\\embeddings\\openai.py:120\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openai\\resources\\embeddings.py:128\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    122\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    123\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openai\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openai\\_base_client.py:919\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openai\\_base_client.py:1008\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1007\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openai\\_base_client.py:1057\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openai\\_base_client.py:1008\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1007\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openai\\_base_client.py:1057\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\openai\\_base_client.py:1023\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1022\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1026\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1027\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1032\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "#RAFT-hoz megfelelő adatok generálása\n",
    "raft_training_data = []\n",
    "skipped_count = 0\n",
    "\n",
    "for item in repo_data_train:\n",
    "    question = item['question']\n",
    "    original_answer = item['answer']\n",
    "    index = item['index']\n",
    "    all_document_chunks = item['documents']\n",
    "\n",
    "    #Először meg kell találni a D* dokumentumot\n",
    "    D_star_doc = find_D_star_document(question, original_answer, all_document_chunks, OpenAIEmbeddings(), index)\n",
    "    if not D_star_doc:\n",
    "        print(f\"A '{question} ' kérdés kihagyása. D* dokumentum nem található.\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    #CoT válasz generálása\n",
    "    cot_answer_str = generate_cot_answer(question, D_star_doc, original_answer)\n",
    "    if not cot_answer_str:\n",
    "        print(f\"A '{question} ' kérdés kihagyása.  CoT generálás sikertelen.\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    #Distractor dokumentumok kiválasztása\n",
    "    distractor_docs = select_distractor_documents(question, D_star_doc, all_document_chunks, OpenAIEmbeddings(), index, k=NUM_DISTRACTORS)\n",
    "\n",
    "    #A D* benne legyen-e a válaszban egy adott elemnél\n",
    "    include_D_star = random.random() < PERCENT_D_STAR_INCLUDED\n",
    "\n",
    "    context_docs_for_instance = []\n",
    "    if include_D_star:\n",
    "        #D* dokumentum benne van a válaszban\n",
    "        context_docs_for_instance.append(D_star_doc)\n",
    "        context_docs_for_instance.extend(distractor_docs)\n",
    "    else:\n",
    "        #Csak a distractor dokumentumokat tartalmazza\n",
    "        context_docs_for_instance.extend(distractor_docs)\n",
    "\n",
    "\n",
    "    random.shuffle(context_docs_for_instance) #a dokumentumok megkeverése\n",
    "\n",
    "    context_string = \"\\n\\n\".join(context_docs_for_instance) #A kontexus létrehozása\n",
    "\n",
    "    #fine-tuning-hez megfelelő formátum létrehozása \n",
    "    formatted_input = f\"Question: {question}\\n\\nContext:\\n{context_string}\"\n",
    "    formatted_output = cot_answer_str #CoT válasz\n",
    "\n",
    "    raft_training_data.append({\n",
    "        \"input\": formatted_input,\n",
    "        \"output\": formatted_output\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "41e18a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'Question: In the neighbour finding section, why does the code calculate the angle between the source-particle vector and the source-neighbour vector instead of just selecting the closest neighbour that is also closer to the source?\\n\\nContext:\\nfor (ii=0;ii<N;ii++) \\t\\t\\t\\t//Loop over all particles\\r\\n\\t{\\r\\n\\r\\n\\tfor (pp=0;pp<nos;pp++)\\t\\t//For each source\\r\\n\\t\\t{\\r\\n\\t\\t//For each particle that considers me a neighbour\\r\\n\\t\\tfor (jj=0;jj<ionisedsph[ii].neighstorcont;jj++)\\r\\n\\t\\t\\t{\\r\\n\\t\\t\\t//Work out the distances for both the test and candidate particle\\r\\n\\t\\t\\tdistanceii=sqrt(pow(ionisedsph[ii].x-ionisedsph[sinkid[pp]].x,2.)+pow(ionisedsph[ii].y-ionisedsph[sinkid[pp]].y,2.)+pow(ionisedsph[ii].z-ionisedsph[sinkid[pp]].z,2.));\\n\\nif (angletest<ionisedsph[ii].angle[pp])\\r\\n\\t\\t\\t\\t\\t{\\r\\n\\t\\t\\t\\t\\tionisedsph[ii].angle[pp]=angletest;\\t\\t\\t\\t//Set new comparison angle to be that of the neighbour\\r\\n\\t\\t\\t\\t\\tionisedsph[ii].neigh[pp]=ionisedsph[ii].neighstor[jj];\\t//Write particle id to neigh array\\r\\n\\t\\t\\t\\t\\t}\\r\\n\\t\\t\\t\\t}\\r\\n\\t\\t\\t}\\r\\n\\r\\n\\t\\t}\\r\\n\\t}\\n\\ndouble distanceii,distancejj,temp_radius,temp_radius2;\\t//Distance between the source of the test particle ii and the neighbour particle jj\\r\\n\\r\\n//Begin neighbour find\\r\\n#pragma omp parallel for private(current_paricle_nn,ii,jj,tt,Nneigb,temp_radius,temp_radius2,dot,mag,angletest,pp,distanceii,distancejj) //Initiate openmp\\n\\nTitle: Periodic gravity\\nDescription: There is still a small bug in the periodic gravity routine. This is related to the fact that the nearest periodic neighbour to a cell centre is not necessarily the same as the one closest to a particle within the cell. \\n\\nThis means that we need to make sure that we have the periodic corrected distance on a particle by particle basis.\\n\\ndistancejj=sqrt(pow(ionisedsph[ionisedsph[ii].neighstor[jj]].x-ionisedsph[sinkid[pp]].x,2.)+pow(ionisedsph[ionisedsph[ii].neighstor[jj]].y-ionisedsph[sinkid[pp]].y,2.)+pow(ionisedsph[ionisedsph[ii].neighstor[jj]].z-ionisedsph[sinkid[pp]].z,2.));\\r\\n\\t\\t\\t//If the candidate particle is closer than the test particle it is a candidate (Also has controle so a particle cant be its own neighbour)\\r\\n\\t\\t\\tif (distancejj<distanceii and ii!=ionisedsph[ii].neighstor[jj])\\r\\n\\t\\t\\t\\t{',\n",
       "  'output': '##Reason: The context does not provide a direct explanation as to why the code calculates the angle between the source-particle vector and the source-neighbour vector instead of just selecting the closest neighbour that is also closer to the source. However, it does show that the code sets a new comparison angle to be that of the neighbour and writes the particle id to the neigh array. This suggests that the code is interested in finding the neighbour that lies closest to the direct line-of-sight path between the particle and the source, rather than just the nearest particle. This is likely done to build a chain for subsequent photon absorption calculations along that path, as indicated by the original answer.\\n##Answer: The code calculates the angle to find the neighbour that lies closest to the direct line-of-sight path between the particle and the source, rather than just the nearest particle, to build a chain for subsequent photon absorption calculations along that path.'},\n",
       " {'input': \"Question: What is the purpose of the REDIRECT_TO_V1 dictionary?\\n\\nContext:\\nHaving this work consistently would be nice for defining dictionaries with certain required keys, but allowing for arbitrary extras.\\n\\nREDIRECT_TO_V1 = {\\n    f'pydantic.utils:{obj}': f'pydantic.v1.utils:{obj}'\\n    for obj in (\\n        'deep_update',\\n        'GetterDict',\\n        'lenient_issubclass',\\n        'lenient_isinstance',\\n        'is_valid_field',\\n        'update_not_none',\\n        'import_string',\\n        'Representation',\\n        'ROOT_KEY',\\n        'smart_deepcopy',\\n        'sequence_like',\\n    )\\n}\\n\\n- It's a change in comparison with `V1`\\r\\n- We need users' feedback/requests on it to decide about the behavior\\r\\n\\r\\nSo, there are two possible options:\\r\\n\\r\\n- we just allow this to fine, or\\r\\n- we add a switch to prevent access (`__getattr__` and `__setattr__`) on extra attributes which match a protected namespace\\n\\n## v1.1 (2019-11-07)\\n\\n* ♻️ Use different error message on v1 redirects by @Kludex in [#6595](https://github.com/pydantic/pydantic/pull/6595)\\n* ⬆ Upgrade `pydantic-core` to v2.2.0 by @lig in [#6589](https://github.com/pydantic/pydantic/pull/6589)\\n* Fix serialization for IPvAny by @dmontagu in [#6572](https://github.com/pydantic/pydantic/pull/6572)\\n* Improve CI by using PDM instead of pip to install typing-extensions by @adriangb in [#6602](https://github.com/pydantic/pydantic/pull/6602)\",\n",
       "  'output': '##Reason: The context provides a Python dictionary named REDIRECT_TO_V1. This dictionary maps certain utility functions from the main Pydantic namespace to their v1 implementations. The purpose of this dictionary is not explicitly stated in the context, but it can be inferred from the structure and content of the dictionary. The dictionary keys are the names of utility functions in the main Pydantic namespace, and the corresponding values are the names of the same functions in the v1 namespace. This suggests that the dictionary is used to redirect calls to these functions from the main namespace to the v1 namespace. This is a common technique used in software development to maintain backward compatibility when transitioning between major versions of a software package. The dictionary allows old code that uses the main namespace to continue to work by redirecting calls to the v1 namespace. At the same time, it can generate a warning message to inform users about the change and encourage them to update their code to use the newer patterns in the v1 namespace.\\n##Answer: The REDIRECT_TO_V1 dictionary automatically redirects certain utility functions from the main Pydantic namespace to their v1 implementations, generating a warning message that informs users about the change while maintaining backward compatibility. This helps manage the transition between major versions by ensuring old code continues to work while encouraging updates to newer patterns.'},\n",
       " {'input': 'Question: The flash function contains a comment explaining that the original implementation using session.setdefault(\\'_flashes\\', []).append(...) had issues. what was this issue, and how does the current implementation address it?\\n\\nContext:\\nimport flask\\n\\nflask.message_flashed.connect(record, app)\\n    try:\\n        client = app.test_client()\\n        with client.session_transaction():\\n            client.get(\"/\")\\n            assert len(recorded) == 1\\n            message, category = recorded[0]\\n            assert message == \"This is a flash message\"\\n            assert category == \"notice\"\\n    finally:\\n        flask.message_flashed.disconnect(record, app)\\n\\nwith client:\\n        client.get(\"/\")\\n        s = flask.session\\n        assert s[\"t\"] == (1, 2, 3)\\n        assert type(s[\"b\"]) is bytes  # noqa: E721\\n        assert s[\"b\"] == b\"\\\\xff\"\\n        assert type(s[\"m\"]) is Markup  # noqa: E721\\n        assert s[\"m\"] == Markup(\"<html>\")\\n        assert s[\"u\"] == the_uuid\\n        assert s[\"d\"] == now\\n        assert s[\"t_tag\"] == {\" t\": \"not-a-tuple\"}\\n        assert s[\"di_t_tag\"] == {\" t__\": \"not-a-tuple\"}\\n\\n#     session.setdefault(\\'_flashes\\', []).append((category, message))\\n    #\\n    # This assumed that changes made to mutable structures in the session are\\n    # always in sync with the session object, which is not true for session\\n    # implementations that use external storage for keeping their keys/values.\\n    flashes = session.get(\"_flashes\", [])\\n    flashes.append((category, message))\\n    session[\"_flashes\"] = flashes\\n    app = current_app._get_current_object()  # type: ignore\\n\\ndef test_flashes(app, req_ctx):\\n    assert not flask.session.modified\\n    flask.flash(\"Zap\")\\n    flask.session.modified = False\\n    flask.flash(\"Zip\")\\n    assert flask.session.modified\\n    assert list(flask.get_flashed_messages()) == [\"Zap\", \"Zip\"]',\n",
       "  'output': '##Reason: The context explains that the original implementation of the flash function, ##begin_quote##session.setdefault(\\'_flashes\\', []).append((category, message))##end_quote##, had a problem. The issue was that it assumed that changes made to mutable structures in the session are always in sync with the session object. However, this is not the case for session implementations that use external storage for keeping their keys/values. This is stated in the context: ##begin_quote##This assumed that changes made to mutable structures in the session are always in sync with the session object, which is not true for session implementations that use external storage for keeping their keys/values.##end_quote##. The current implementation addresses this issue by explicitly retrieving the list using session.get(_flashes, []), appending the new message to this retrieved list, and then explicitly assigning the modified list back to the session via session[_flashes] = flashes. This is shown in the context: ##begin_quote##flashes = session.get(\"_flashes\", [])\\n    flashes.append((category, message))\\n    session[\"_flashes\"] = flashes##end_quote##. This ensures that the change is correctly persisted regardless of the session backend.\\n##Answer: According to the comment within the flash function, the original implementation assumed that modifying the list obtained via session.setdefault() would always update the session object itself. This assumption is incorrect for session implementations that use external storage (where the session object and its stored data might not be the same in-memory object). The current implementation addresses this by explicitly retrieving the list using session.get(_flashes, []), appending the new message to this retrieved list, and then explicitly assigning the modified list back to the session via session[_flashes] = flashes, ensuring the change is correctly persisted regardless of the session backend.'},\n",
       " {'input': 'Question: What is the purpose of the gamma parameter in the RiemannSolver?\\n\\nContext:\\nclass ExactRiemannSolver: public RiemannSolver<ndim>\\n{\\n  using RiemannSolver<ndim>::gamma;\\n  using RiemannSolver<ndim>::g1;\\n  using RiemannSolver<ndim>::g2;\\n  using RiemannSolver<ndim>::g3;\\n  using RiemannSolver<ndim>::g4;\\n  using RiemannSolver<ndim>::g5;\\n  using RiemannSolver<ndim>::g6;\\n  using RiemannSolver<ndim>::g7;\\n  using RiemannSolver<ndim>::g8;\\n  using RiemannSolver<ndim>::g9;\\n  using RiemannSolver<ndim>::invgamma;\\n  using RiemannSolver<ndim>::zeroMassFlux;\\n\\nstatic const int nvar = ndim + 2;\\n  static const int ivx = 0;\\n  static const int ivy = 1;\\n  static const int ivz = 2;\\n  static const int irho = ndim;\\n  static const int ietot = ndim + 1;\\n  static const int ipress = ndim + 1;\\n\\n public:\\n\\n  ExactRiemannSolver(FLOAT gamma_aux, bool _zeroMassFlux) : RiemannSolver<ndim>(gamma_aux, _zeroMassFlux) {};\\n  virtual ~ExactRiemannSolver() {};\\n\\n//=================================================================================================\\n//  RiemannSolver::RiemannSolver\\n/// Constructor for RiemannSolver class.\\n//=================================================================================================\\ntemplate <int ndim>\\nRiemannSolver<ndim>::RiemannSolver(FLOAT gamma_aux, bool _zeroMassFlux):\\n  gamma(gamma_aux),\\n  invgamma(1.0/gamma_aux),\\n  g1(0.5*(gamma_aux - 1.0)/gamma_aux),\\n  g2(0.5*(gamma_aux + 1.0)/gamma_aux),\\n\\n// Riemann solver object\\n  //-----------------------------------------------------------------------------------------------\\n  string riemann_solver = stringparams[\"riemann_solver\"];\\n  if (riemann_solver == \"exact\") {\\n    RiemannSolverType = exact ;\\n  }\\n  else if (riemann_solver == \"hllc\") {\\n   RiemannSolverType = hllc ;\\n  }\\n  else {\\n    string message = \"Unrecognised parameter : riemann_solver = \" + riemann_solver;\\n    ExceptionHandler::getIstance().raise(message);\\n  }\\n\\n}\\n\\nriemannExact(gamma_aux, params->intparams[\"zero_mass_flux\"]),\\n  riemannHLLC(gamma_aux, params->intparams[\"zero_mass_flux\"], gas_eos_aux == \"isothermal\"),\\n  viscosity(params->floatparams[\"shear_visc\"], params->floatparams[\"bulk_visc\"]),\\n  need_viscosity(params->floatparams[\"shear_visc\"] || params->floatparams[\"bulk_visc\"])\\n{\\n  this->kernp      = &kern;\\n  this->kernrange  = this->kernp->kernrange;',\n",
       "  'output': \"##Reason: The context does not provide a direct explanation of the purpose of the gamma parameter in the RiemannSolver. However, it does show that the gamma parameter is used in the calculation of other parameters in the RiemannSolver class. For example, it is used in the calculation of ##begin_quote##invgamma(1.0/gamma_aux)##end_quote##, ##begin_quote##g1(0.5*(gamma_aux - 1.0)/gamma_aux)##end_quote##, and ##begin_quote##g2(0.5*(gamma_aux + 1.0)/gamma_aux)##end_quote##. This suggests that the gamma parameter is an important part of the mathematical model used in the RiemannSolver, and given the name 'gamma', it is likely to represent the adiabatic index in the model, which is a parameter used in fluid dynamics and thermodynamics to describe the properties of a specific gas.\\n##Answer: Adiabatic index\"},\n",
       " {'input': 'Question: Does this program support Python 3.14?\\n\\nContext:\\npython version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\\r\\n                     platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\\r\\n             related packages: typing_extensions-4.10.0 mypy-1.9.0 pydantic-settings-2.2.1\\r\\n                       commit: unknown\\r\\n```\\n\\npython version: 3.11.8 (main, Feb 25 2024, 16:41:26) [GCC 9.4.0]\\r\\n                     platform: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\\r\\n             related packages: typing_extensions-4.10.0\\r\\n                       commit: unknown\\n```\\n\\npython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\\r\\n                     platform: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\\r\\n             related packages: mypy-1.8.0 typing_extensions-4.9.0\\r\\n                       commit: unknown\\r\\n```\\n\\npython version: 3.12.7 (main, Oct  1 2024, 11:15:50) [GCC 14.2.1 20240910]\\r\\n                     platform: Linux-6.11.5-arch1-1-x86_64-with-glibc2.40\\r\\n             related packages: typing_extensions-4.12.2 fastapi-0.115.0 mypy-1.13.0 pydantic-settings-2.6.1\\r\\n                       commit: unknown\\r\\n```',\n",
       "  'output': '##Reason: The context mentions that the ##begin_quote## First 3.14 beta release is planned on 2025-05-06 ##end_quote##. This indicates that the Python 3.14 version is not yet released and thus, the program cannot currently support it. The context also states that ##begin_quote## we should add support to 3.14 and report any bugs/issues ##end_quote##, which implies that the program does not currently support Python 3.14, but there are plans to add support for it in the future.\\n##Answer: No, this program supports Python 3.14. It is only planned for the future.'},\n",
       " {'input': \"Question: What is the purpose of the is_root_model function?\\n\\nContext:\\n```\\n\\nNote also that [`RootModel`](models.md#rootmodel-and-custom-root-types) _does_ get converted to a dictionary with the key `'root'`.\\n\\n::: pydantic.root_model\\n\\nNote that for [root models](#rootmodel-and-custom-root-types), the root value can be passed to\\n[`model_construct()`][pydantic.main.BaseModel.model_construct] positionally, instead of using a keyword argument.\\n\\nHere are some additional notes on the behavior of [`model_construct()`][pydantic.main.BaseModel.model_construct]:\\n\\n```\\n\\nThis trick is actually used in [`RootModel`](models.md#rootmodel-and-custom-root-types) for precisely this purpose.\",\n",
       "  'output': '##Reason: The purpose of the is_root_model function is explained in the context. The function is defined as returning whether the type info is a root model subclass or the `RootModel` class itself. This is done by checking if the type has a base class matching the ROOT_MODEL_FULLNAME constant. This is indicated in the context by the line ##begin_quote##\"Return whether the type info is a root model subclass (or the `RootModel` class itself).\"##end_quote## and ##begin_quote##\"return info.has_base(ROOT_MODEL_FULLNAME)\"##end_quote##.\\n##Answer: The is_root_model function determines whether a given TypeInfo represents a RootModel subclass or the RootModel class itself. It does this by checking if the type has a base class matching the ROOT_MODEL_FULLNAME constant, which is defined as \\'pydantic.root_model.RootModel\\'.'},\n",
       " {'input': \"Question: What is Flask\\n\\nContext:\\nFlask offers suggestions, but doesn't enforce any dependencies or\\nproject layout. It is up to the developer to choose the tools and\\nlibraries they want to use. There are many extensions provided by the\\ncommunity that make adding new functionality easy.\\n\\n[WSGI]: https://wsgi.readthedocs.io/\\n[Werkzeug]: https://werkzeug.palletsprojects.com/\\n[Jinja]: https://jinja.palletsprojects.com/\\n\\nimport flask\\n\\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n---\\nname: Feature request\\nabout: Suggest a new feature for Flask\\n---\",\n",
       "  'output': '##Reason: The context provides a clear definition of Flask. It states that ##begin_quote## Flask is a lightweight [WSGI] web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. It began as a simple wrapper around [Werkzeug] and [Jinja], and has become one of the most popular Python web application frameworks. ##end_quote## This sentence provides all the necessary information to answer the question.\\n##Answer: Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. It began as a simple wrapper around Werkzeug and Jinja, and has become one of the most popular Python web application frameworks.'},\n",
       " {'input': 'Question: Based on the parameters read from the configuration file, how does the ProcessSphParameters function determine which specific SPH kernel class (like M4Kernel, QuinticKernel, GaussianKernel, or TabulatedKernel) should be instantiated for the sph object?\\n\\nContext:\\ntemplate class SM2012Sph<1, M4Kernel>;\\ntemplate class SM2012Sph<1, QuinticKernel>;\\ntemplate class SM2012Sph<1, GaussianKernel>;\\ntemplate class SM2012Sph<1, TabulatedKernel>;\\ntemplate class SM2012Sph<2, M4Kernel>;\\ntemplate class SM2012Sph<2, QuinticKernel>;\\ntemplate class SM2012Sph<2, GaussianKernel>;\\ntemplate class SM2012Sph<2, TabulatedKernel>;\\ntemplate class SM2012Sph<3, M4Kernel>;\\ntemplate class SM2012Sph<3, QuinticKernel>;\\ntemplate class SM2012Sph<3, GaussianKernel>;\\n\\navisc, acond, tdavisc, stringparams[\"gas_eos\"], KernelName, simunits, simparams);\\n  }\\n  else if (intparams[\"tabulated_kernel\"] == 0) {\\n    // Depending on the kernel, instantiate a different GradSph object\\n    if (KernelName == \"m4\") {\\n      sph = new GradhSph<ndim, M4Kernel>\\n        (intparams[\"hydro_forces\"], intparams[\"self_gravity\"], floatparams[\"alpha_visc\"],\\n         floatparams[\"beta_visc\"], floatparams[\"h_fac\"], floatparams[\"h_converge\"],\\n\\nsph = new GradhSph<ndim, GaussianKernel>\\n        (intparams[\"hydro_forces\"], intparams[\"self_gravity\"], floatparams[\"alpha_visc\"],\\n         floatparams[\"beta_visc\"], floatparams[\"h_fac\"], floatparams[\"h_converge\"],\\n         avisc, acond, tdavisc, stringparams[\"gas_eos\"], KernelName, simunits, simparams);\\n    }\\n    else {\\n      string message = \"Unrecognised parameter : kernel = \" + simparams->stringparams[\"kernel\"];\\n      ExceptionHandler::getIstance().raise(message);\\n    }\\n  }\\n  else {\\n\\nsph = new SM2012Sph<ndim, GaussianKernel>\\n\\t  (intparams[\"hydro_forces\"], intparams[\"self_gravity\"],\\n\\t   floatparams[\"alpha_visc\"], floatparams[\"beta_visc\"],\\n\\t   floatparams[\"h_fac\"], floatparams[\"h_converge\"],\\n\\t   avisc, acond, tdavisc, stringparams[\"gas_eos\"], KernelName, simunits, simparams);\\n      }\\n      else {\\n\\tstring message = \"Unrecognised parameter : kernel = \" +\\n\\t  simparams->stringparams[\"kernel\"];\\n\\tExceptionHandler::getIstance().raise(message);\\n      }\\n    }\\n    else {',\n",
       "  'output': 'Reason: The context provides the logic used by the ProcessSphParameters function to determine which SPH kernel class should be instantiated. The function first checks the value of the \"tabulated_kernel\" parameter. This is evident from the line ##begin_quote##else if (intparams[\"tabulated_kernel\"] == 0)##end_quote##. If this parameter is equal to zero, the function then checks the value of the \"KernelName\" parameter. Depending on the value of \"KernelName\", a different SPH kernel class is instantiated. This is evident from the lines ##begin_quote##if (KernelName == \"m4\")##end_quote## and ##begin_quote##else if (KernelName == \"quintic\")##end_quote##. The context does not provide information about the GaussianKernel, so we can only infer that the function chooses between M4Kernel and QuinticKernel based on the provided context.\\nAnswer: The function selects the SPH kernel class by first checking the tabulated_kernel integer parameter, and if it\\'s zero, it then uses the kernel string parameter to choose between M4Kernel and QuinticKernel.'}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " raft_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca7ebf",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bba6dcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konvertálva 8\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "openai_formatted_filename = \"openai_formatted_training_data.jsonl\"\n",
    "base_model = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "\n",
    "#Konverzió OpenAI chat formátumra\n",
    "\n",
    "converted_count = 0\n",
    "with open(openai_formatted_filename, 'w', encoding='utf-8') as outfile:\n",
    "    for raft_example in raft_training_data:\n",
    "        \n",
    "        user_content = raft_example.get(\"input\")\n",
    "        assistant_content = raft_example.get(\"output\")\n",
    "\n",
    "        if not user_content or not assistant_content:\n",
    "            print(f\" Az 'input' vagy 'output' üres: {raft_example}\")\n",
    "            continue\n",
    "\n",
    "        openai_message = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        outfile.write(json.dumps(openai_message) + '\\n')\n",
    "        converted_count += 1\n",
    "\n",
    "print(f\"Konvertálva {converted_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2a626983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fájl feltöltése fine-tuninghoz\n",
    "\n",
    "with open(openai_formatted_filename, \"rb\") as f:\n",
    "    training_file = client.files.create(\n",
    "        file=f,\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "training_file_id = training_file.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bc17ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-Tuning Job\n",
    "\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    model=base_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7828f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A fine tuning folyamatának figyelése\n",
    "job_id = job.id\n",
    "job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "status = job_status.status\n",
    "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Job Status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "66e3d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model_id = job_status.fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910df8bb",
   "metadata": {},
   "source": [
    "## Fine-tuning + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826717ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_raft = ChatOpenAI(model_name=fine_tuned_model_id, temperature=0)\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_raft = prompt | llm_raft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae263b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tesztekés egy kérdésen\n",
    "\n",
    "sample_query = repo_data_test[0]['question']\n",
    "print(f\" Kérdés: {sample_query}\")\n",
    "\n",
    "sample_relevant_docs = retrieve_relevant_document(sample_query, index, documents, OpenAIEmbeddings())\n",
    "sample_context_str = \"\\n\\n\".join(sample_relevant_docs)\n",
    "\n",
    "response_raft = chain_raft.invoke({\n",
    "    \"context\": sample_context_str,\n",
    "    \"question\": sample_query\n",
    "})\n",
    "\n",
    "\n",
    "print(response_raft.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22569b4f",
   "metadata": {},
   "source": [
    "## Kiértékelés RAGAs-sal a teszthalmazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c30a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_data(chain):\n",
    "    questions = [item[\"question\"] for item in repo_data_test]\n",
    "    ground_truths = [item[\"answer\"] for item in repo_data_test]\n",
    "    answers = []\n",
    "    contexts = []\n",
    "\n",
    "    for query in questions:\n",
    "        context = retrieve_relevant_document(query, index, documents, OpenAIEmbeddings())\n",
    "        answers.append(chain.invoke({\"context\": context,\"question\":query}).content)\n",
    "        contexts.append(context)\n",
    "\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"reference\": ground_truths\n",
    "    }\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df = result.to_pandas()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510bb6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = evaluate_test_data(chain)\n",
    "df_raft = evaluate_test_data(chain_raft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c8930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
