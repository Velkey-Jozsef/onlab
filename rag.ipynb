{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install -U langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'api_key' #kicseréltem, hogy a githubon ne legyen benne a kulcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28175e-24b6-4939-8a3c-5a1f9511f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyGithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2502a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec0a82",
   "metadata": {},
   "source": [
    "## Github repo feldolgozása és indexelés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98eff590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from github import Github\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d88407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GitHub Repoból kiszedi a szükséges dolgokat (kód, dokumentáció)\n",
    "def clone_repo(github_token, repo_name):\n",
    "    g = Github(github_token)\n",
    "    repo = g.get_repo(repo_name)\n",
    "    \n",
    "    contents = repo.get_contents(\"\")\n",
    "    \n",
    "    files = []\n",
    "    while contents:\n",
    "        file_content = contents.pop(0)\n",
    "        if file_content.type == \"dir\":\n",
    "            contents.extend(repo.get_contents(file_content.path))\n",
    "        else:\n",
    "            if file_content.name.endswith(('.md', '.py', '.txt', '.json', '.yml', '.c', '.h', '.cpp', '.hpp')): # Ezeket a fájlokat fogadja el egyenlőre\n",
    "                file_data = {\n",
    "                    \"file_name\": file_content.name,\n",
    "                    \"file_content\": requests.get(file_content.download_url).text\n",
    "                }\n",
    "                files.append(file_data)\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8dc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Issue-k kiszedése a repo-ból\n",
    "def fetch_issues(github_token, repo_name):\n",
    "    g = Github(github_token)\n",
    "    repo = g.get_repo(repo_name)\n",
    "    \n",
    "    issues = []\n",
    "    for issue in repo.get_issues(state='open'):\n",
    "        issue_data = {\n",
    "            \"file_name\": f\"issue_{issue.number}\",\n",
    "            \"file_content\": f\"Title: {issue.title}\\nDescription: {issue.body}\"\n",
    "        }\n",
    "        issues.append(issue_data)\n",
    "    \n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd639ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A fájlok kisebb darabokra bontása\n",
    "def preprocess_files(files):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "    documents = []\n",
    "    \n",
    "    for file in files:\n",
    "        content = file['file_content']\n",
    "        \n",
    "        #Markdown fájlok esetén HTML kód eltávolítása\n",
    "        if file['file_name'].endswith(('.md', '.html')):\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            content = soup.get_text()\n",
    "        \n",
    "        chunks = text_splitter.split_text(content)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            documents.append(chunk)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3b3e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddingek generálása  OpenAI embeddings-el és FAISS indexeléssel\n",
    "def create_embeddings(documents):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    doc_embeddings = embeddings.embed_documents(documents)\n",
    "    doc_embeddings_np = np.array(doc_embeddings)\n",
    "    \n",
    "    index = faiss.IndexFlatL2(doc_embeddings_np.shape[1])\n",
    "    index.add(doc_embeddings_np)\n",
    "    \n",
    "    return index, doc_embeddings_np, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Egy konkrét repora alkalmazva a fenti funkciókat\n",
    "\n",
    "github_token = \"api_key\"\n",
    "repo_name = \"pydantic/pydantic\"\n",
    "    \n",
    "files = clone_repo(github_token, repo_name)\n",
    "    \n",
    "issues = fetch_issues(github_token, repo_name)\n",
    "    \n",
    "all_documents = files + issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = preprocess_files(all_documents)\n",
    "    \n",
    "index, doc_embeddings_np, documents = create_embeddings(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8cf6e",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "354cf5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A kérdés (Query) feldolgozása és a legrelevánsabb dokumentumok visszaadása\n",
    "def retrieve_relevant_document(query, index, documents, embeddings, k=5):\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    query_embedding_np = np.array(query_embedding).reshape(1, -1)\n",
    "    D, I = index.search(query_embedding_np, k)\n",
    "    \n",
    "    relevant_docs = [documents[i] for i in I[0]]\n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7bd2000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 válasz legrevelánsabb dokumentum:\n",
      "- Title: Add support for Python 3.14\n",
      "Description: First 3.14 beta release is [planned on 2025-05-06](https://peps.python.org/pep-0745/#release-schedule) and PEP 649/749 is almost fully implemented.\n",
      "\n",
      "Considering the significant changes it provides to the runtime evaluation of type hints, we should add support to 3.14 and report any bugs/issues.\n",
      "- ### Python, Pydantic & OS Version\n",
      "\n",
      "```Text\n",
      "sqlalchemy 1.4\n",
      "pydantic 2.7.1\n",
      "python 3.11.5\n",
      "```\n",
      "- ```Python\n",
      "\n",
      "```\n",
      "\n",
      "### Python, Pydantic & OS Version\n",
      "\n",
      "```Text\n",
      "2.10\n",
      "```\n",
      "- ```Text\n",
      "Python 3.10.9\n",
      "pydantic 2.0a4\n",
      "Windows 10\n",
      "```\n",
      "- As it is being proposed as an official standard for Python, other editors can also easily add support for it.\n",
      "\n",
      "And authors of other libraries similar to Pydantic can also easily adopt the standard right away (using the \"Alternate Form\") and get the benefits of these additional editor features.\n"
     ]
    }
   ],
   "source": [
    "#Példa kérdés\n",
    "query = \"Does this program support Python 3.14?\"\n",
    "relevant_docs = retrieve_relevant_document(query, index, documents, OpenAIEmbeddings())\n",
    "    \n",
    "print(f\"5 válasz legrevelánsabb dokumentum:\")\n",
    "for doc in relevant_docs:\n",
    "        print(f\"- {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4461264-5cac-479a-917c-9bf589826da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, the program supports Python 3.14 as it mentions adding support for Python 3.14 and reporting any bugs/issues related to it.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 287, 'total_tokens': 318, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BI0hTQc1k9Jy3InGqaLvXchGYEUnI', 'finish_reason': 'stop', 'logprobs': None}, id='run-e0b0224f-cc5a-4f5c-bd2a-23c122fe794c-0', usage_metadata={'input_tokens': 287, 'output_tokens': 31, 'total_tokens': 318, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\":relevant_docs,\"question\":\"Does this program support Python 3.14?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
