{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install -U langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28175e-24b6-4939-8a3c-5a1f9511f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MISTRAL_API_KEY'] = 'api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = 'api_key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_token = \"api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyGithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2502a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec0a82",
   "metadata": {},
   "source": [
    "## Github repo feldolgozása és indexelés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98eff590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from github import Github\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d88407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GitHub Repoból kiszedi a szükséges dolgokat (kód, dokumentáció)\n",
    "def clone_repo(github_token, repo_name):\n",
    "    g = Github(github_token)\n",
    "    repo = g.get_repo(repo_name)\n",
    "    \n",
    "    contents = repo.get_contents(\"\")\n",
    "    \n",
    "    files = []\n",
    "    while contents:\n",
    "        file_content = contents.pop(0)\n",
    "        if file_content.type == \"dir\":\n",
    "            contents.extend(repo.get_contents(file_content.path))\n",
    "        else:\n",
    "            if file_content.name.endswith(('.md', '.py', '.txt', '.json', '.yml', '.c', '.h', '.cpp', '.hpp')): # Ezeket a fájlokat fogadja el egyenlőre\n",
    "                file_data = {\n",
    "                    \"file_name\": file_content.name,\n",
    "                    \"file_content\": requests.get(file_content.download_url).text\n",
    "                }\n",
    "                files.append(file_data)\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8dc4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Issue-k kiszedése a repo-ból\n",
    "def fetch_issues(github_token, repo_name):\n",
    "    g = Github(github_token)\n",
    "    repo = g.get_repo(repo_name)\n",
    "    \n",
    "    issues = []\n",
    "    for issue in repo.get_issues(state='open'):\n",
    "        issue_data = {\n",
    "            \"file_name\": f\"issue_{issue.number}\",\n",
    "            \"file_content\": f\"Title: {issue.title}\\nDescription: {issue.body}\"\n",
    "        }\n",
    "        issues.append(issue_data)\n",
    "    \n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd639ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A fájlok kisebb darabokra bontása\n",
    "def preprocess_files_old(files):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "    documents = []\n",
    "    \n",
    "    for file in files:\n",
    "        content = file['file_content']\n",
    "        \n",
    "        #Markdown fájlok esetén HTML kód eltávolítása\n",
    "        if file['file_name'].endswith(('.md', '.html')):\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            content = soup.get_text()\n",
    "        \n",
    "        chunks = text_splitter.split_text(content)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            documents.append(chunk)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c0f266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "EXTENSION_LANGUAGE_MAP = {\n",
    "    '.py': Language.PYTHON,\n",
    "    '.cpp': Language.CPP,\n",
    "    '.c': Language.C,\n",
    "    '.cs': Language.CSHARP,\n",
    "    '.md': Language.MARKDOWN,\n",
    "    '.html': Language.HTML,\n",
    "}\n",
    "\n",
    "def get_language_from_filename(filename):\n",
    "    for ext, lang in EXTENSION_LANGUAGE_MAP.items():\n",
    "        if filename.endswith(ext):\n",
    "            return lang\n",
    "    return None\n",
    "\n",
    "def preprocess_files(files):\n",
    "    documents = []\n",
    "    \n",
    "    for file in files:\n",
    "        filename = file['file_name']\n",
    "        content = file['file_content']\n",
    "\n",
    "        if filename.endswith(('.md', '.html')):\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            content = soup.get_text()\n",
    "\n",
    "        lang = get_language_from_filename(filename)\n",
    "\n",
    "        if lang:\n",
    "            splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "                language=lang, chunk_size=500, chunk_overlap=0\n",
    "            )\n",
    "        else:\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "        docs = splitter.create_documents([content])\n",
    "        documents.extend(doc.page_content for doc in docs)\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3b3e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddingek generálása  OpenAI embeddings-el és FAISS indexeléssel\n",
    "def create_embeddings(documents):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    doc_embeddings = embeddings.embed_documents(documents)\n",
    "    doc_embeddings_np = np.array(doc_embeddings)\n",
    "    \n",
    "    index = faiss.IndexFlatL2(doc_embeddings_np.shape[1])\n",
    "    index.add(doc_embeddings_np)\n",
    "    \n",
    "    return index, doc_embeddings_np, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Egy konkrét repora alkalmazva a fenti funkciókat\n",
    "\n",
    "github_token = \"api_key\"\n",
    "repo_name = \"pydantic/pydantic\"\n",
    "    \n",
    "files = clone_repo(github_token, repo_name)\n",
    "    \n",
    "issues = fetch_issues(github_token, repo_name)\n",
    "    \n",
    "all_documents = files + issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = preprocess_files(all_documents)\n",
    "    \n",
    "index, doc_embeddings_np, documents = create_embeddings(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8cf6e",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "354cf5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A kérdés (Query) feldolgozása és a legrelevánsabb dokumentumok visszaadása\n",
    "def retrieve_relevant_document(query, index, documents, embeddings, k=5):\n",
    "\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    query_embedding_np = np.array(query_embedding).reshape(1, -1)\n",
    "    D, I = index.search(query_embedding_np, k)\n",
    "    \n",
    "    relevant_docs = [documents[i] for i in I[0]]\n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73661120",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "#Szöveget kisbetűssé alakítja és szóközök mentén feldarabolja\n",
    "def simple_tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "#BM25\n",
    "def build_bm25_index(documents):\n",
    "    tokenized_docs = [simple_tokenize(doc) for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    return bm25, tokenized_docs\n",
    "\n",
    "#Keresés BM25-el\n",
    "def retrieve_relevant_document_bm25(query, bm25, documents, tokenized_docs, k=5):\n",
    "    tokenized_query = simple_tokenize(query)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_k_indices = np.argsort(scores)[::-1][:k]\n",
    "    relevant_docs = [documents[i] for i in top_k_indices]\n",
    "    return relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c126ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Hybrid keresés BM25 és embeddingek kombinálásával\n",
    "def retrieve_relevant_document_hybrid(query, bm25, tokenized_docs, embedding_index, embeddings, documents, k=5, alpha=0.5):\n",
    "    \n",
    "    #BM25\n",
    "    tokenized_query = simple_tokenize(query)\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    #Embedding\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    query_embedding_np = np.array(query_embedding).reshape(1, -1)\n",
    "    D, I = embedding_index.search(query_embedding_np, len(documents))\n",
    "    embedding_scores = np.zeros(len(documents))\n",
    "    for idx, doc_idx in enumerate(I[0]):\n",
    "        embedding_scores[doc_idx] = 1.0 / (1.0 + D[0][idx])  # convert distance to similarity\n",
    "\n",
    "    #Normalizálás\n",
    "    scaler = MinMaxScaler()\n",
    "    bm25_scores_norm = scaler.fit_transform(bm25_scores.reshape(-1, 1)).flatten()\n",
    "    embedding_scores_norm = scaler.fit_transform(embedding_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Kombinálás\n",
    "    combined_scores = alpha * bm25_scores_norm + (1 - alpha) * embedding_scores_norm\n",
    "    top_k_indices = np.argsort(combined_scores)[::-1][:k]\n",
    "\n",
    "    relevant_docs = [documents[i] for i in top_k_indices]\n",
    "    return relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7bd2000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joci\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\langchain_mistralai\\embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 válasz legrevelánsabb dokumentum:\n",
      "- Title: Add support for Python 3.14\n",
      "Description: First 3.14 beta release is [planned on 2025-05-06](https://peps.python.org/pep-0745/#release-schedule) and PEP 649/749 is almost fully implemented.\n",
      "\n",
      "Considering the significant changes it provides to the runtime evaluation of type hints, we should add support to 3.14 and report any bugs/issues.\n",
      "- python version: 3.10.11\n",
      "```\n",
      "- python version: 3.11.5 | packaged by conda-forge | (main, Aug 27 2023, 03:34:09) [GCC 12.3.0]\n",
      "platform: Linux-3.10.0-1160.45.1.el7.x86_64-x86_64-with-glibc2.17\n",
      "related packages: typing_extensions-4.8.0\n",
      "```\n",
      "\n",
      "Update 10/03/23:  Modified the script to be able to run it as a single file\n",
      "- ### Python, Pydantic & OS Version\n",
      "\n",
      "```Text\n",
      "2.11.0b2 but also at least 2.7\n",
      "\n",
      "OS: Apple M2 Mac\n",
      "```\n",
      "- python version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n",
      "                     platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\n",
      "             related packages: typing_extensions-4.10.0 mypy-1.9.0 pydantic-settings-2.2.1\n",
      "                       commit: unknown\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "#Példa kérdés\n",
    "query = \"Does this program support Python 3.14?\"\n",
    "relevant_docs = retrieve_relevant_document(query, index, documents, MistralAIEmbeddings(mistral_api_key=os.environ[\"MISTRAL_API_KEY\"]))\n",
    "    \n",
    "print(f\"5 válasz legrevelánsabb dokumentum:\")\n",
    "for doc in relevant_docs:\n",
    "        print(f\"- {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae9a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt\n",
    "\n",
    "llm_mistral = ChatMistralAI(model_name=\"mistral-small\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e4461264-5cac-479a-917c-9bf589826da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_mistral = ChatMistralAI(model_name=\"mistral-small\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4557c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The context does not provide information about the version compatibility of the program with Python 3.14.', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 35, 'total_tokens': 57, 'completion_tokens': 22}, 'model_name': 'mistral-small', 'model': 'mistral-small', 'finish_reason': 'stop'}, id='run-35ac8675-3036-4d3d-9dbc-8c6867d11f57-0', usage_metadata={'input_tokens': 35, 'output_tokens': 22, 'total_tokens': 57})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\":relevant_docs,\"question\":\"Does this program support Python 3.14?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b341c4",
   "metadata": {},
   "source": [
    "## RAFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a854cd8",
   "metadata": {},
   "source": [
    "## Adathalmaz készítése a RAFT-hoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bacc5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-04-17\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ccaebcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "#Kérdés-válasz párok generálása\n",
    "def generate_qa(context):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=\"You are given context from a software project such as code, documentation, or comments. Generate a useful and self-contained Q&A pair that reflects a realistic question someone might ask about how the software works, how to use it, or how its designed. Focus on generating meaningful technical questions that are relevant to developers, maintainers, or users. The format should be the following:Q: <question>\\nA: <answer>, the context:\\n{context}\\n\",\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm_gemini\n",
    "\n",
    "    response = chain.invoke({\"context\":context})\n",
    "\n",
    "    response = response.content\n",
    "    try:\n",
    "        question_answer = response.strip().split(\"\\nA: \")\n",
    "        question = question_answer[0].replace(\"Q: \", \"\").strip()\n",
    "        answer = question_answer[1].strip()\n",
    "    except:\n",
    "        question, answer = \"Nem sikerült kérdést generálni\", \"Nem sikerült választ generálni\"\n",
    "\n",
    "    return question, answer\n",
    "\n",
    "#A kérdés-válasz párok generálása az összes repo-ra\n",
    "def create_qa_pairs(github_token, repo_list, n_pairs_per_repo=3):\n",
    "    repo_qa_data = {}\n",
    "\n",
    "    for repo_name in repo_list:\n",
    "        print(f\"Feldolgozás alatt: {repo_name}\")\n",
    "\n",
    "        files = clone_repo(github_token, repo_name)\n",
    "        issues = fetch_issues(github_token, repo_name)\n",
    "\n",
    "        print(f\"Feldolgozva: {repo_name}\")\n",
    "        \n",
    "        all_texts = files + issues\n",
    "\n",
    "        \n",
    "        random.shuffle(all_texts) # Randomizálás, hogy random kérdéseket generáljon, ne csak a legelső fájlakból\n",
    "        \n",
    "        qa_pairs = []\n",
    "        \n",
    "        for file_data in all_texts:\n",
    "            if len(qa_pairs) >= n_pairs_per_repo: # Ha elérte a kívánt kérdés-válasz párok számát, akkor kilép\n",
    "                break\n",
    "            context = file_data[\"file_content\"]\n",
    "\n",
    "            context = context[:4000]  #4000 karakterre vágás\n",
    "            \n",
    "            question, answer = generate_qa(context)\n",
    "            \n",
    "            if question and answer:\n",
    "                qa_pairs.append({\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer\n",
    "                })\n",
    "            \n",
    "            time.sleep(5)\n",
    "        \n",
    "        print(qa_pairs)\n",
    "\n",
    "        repo_qa_data[repo_name] = qa_pairs\n",
    "    \n",
    "    return repo_qa_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = [\"pydantic/pydantic\", \"pallets/flask\", \"gandalfcode/gandalf\", \"fmtlib/fmt\", \"prettytable/prettytable\", \"cookiecutter/cookiecutter\", \"git/git\", \"sqlite/sqlite\", \"bitcoin/bitcoin\", \"rom1v/sndcpy\"]\n",
    "repo_qa_data  = create_qa_pairs(github_token, repos, n_pairs_per_repo=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9cf02",
   "metadata": {},
   "source": [
    "Kérdések kimentése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7b31403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"qa_all_pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(repo_qa_data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0341360",
   "metadata": {},
   "source": [
    "Kérdések betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b11ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qa_all_pairs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_qa_data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee70cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A fájlok és issue-k feldolgozása, embeddingek generálása és indexelés\n",
    "def generate_everything(repo_name, github_token):\n",
    "    files = clone_repo(github_token, repo_name)\n",
    "    issues = fetch_issues(github_token, repo_name)\n",
    "    all_documents = files + issues\n",
    "    documents = preprocess_files(all_documents)\n",
    "    index, doc_embeddings_np, documents = create_embeddings(documents)\n",
    "    return index, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281409ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kérdés-válasz párok +indexek és dokumentumok összevonása\n",
    "repo_data_with_index_documents = []\n",
    "\n",
    "for repo_name, qa_pairs in repo_qa_data.items():\n",
    "    index, documents = generate_everything(repo_name, github_token)\n",
    "    for qa in qa_pairs:\n",
    "        repo_data_with_index_documents .append({\n",
    "            \"question\": qa[\"question\"],\n",
    "            \"answer\": qa[\"answer\"],\n",
    "            \"index\": index,\n",
    "            \"documents\": documents\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "37215acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_data_with_index_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2dd2e5",
   "metadata": {},
   "source": [
    "indexelések kimentése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec42ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "\n",
    "repo_data_serializable = []\n",
    "index_map = {}\n",
    "\n",
    "for i, item in enumerate(repo_data_with_index_documents):\n",
    "    index_id = i // 20\n",
    "    index_filename = f\"faiss_index_{index_id}.index\"\n",
    "    \n",
    "    if index_id not in index_map:\n",
    "        faiss.write_index(item[\"index\"], index_filename)\n",
    "        index_map[index_id] = index_filename\n",
    "\n",
    "    item_copy = {\n",
    "        \"question\": item[\"question\"],\n",
    "        \"answer\": item[\"answer\"],\n",
    "        \"documents\": item[\"documents\"],\n",
    "        \"index_file\": index_filename\n",
    "    }\n",
    "    repo_data_serializable.append(item_copy)\n",
    "\n",
    "with open(\"repo_data_serializable.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(repo_data_serializable, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aca6e2",
   "metadata": {},
   "source": [
    "indexelések betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "\n",
    "with open(\"repo_data_serializable.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    repo_data_serializable = json.load(f)\n",
    "\n",
    "index_cache = {}\n",
    "\n",
    "repo_data_reconstructed = []\n",
    "for item in repo_data_serializable:\n",
    "    index_file = item[\"index_file\"]\n",
    "    if index_file not in index_cache:\n",
    "        index_cache[index_file] = faiss.read_index(index_file)\n",
    "    \n",
    "    reconstructed_item = {\n",
    "        \"question\": item[\"question\"],\n",
    "        \"answer\": item[\"answer\"],\n",
    "        \"documents\": item[\"documents\"],\n",
    "        \"index\": index_cache[index_file]\n",
    "    }\n",
    "    repo_data_reconstructed.append(reconstructed_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "5fb19c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train és test adatok szétválasztása\n",
    "import random\n",
    "random.seed(12) \n",
    "shuffled = repo_data_with_index_documents[:]\n",
    "random.shuffle(shuffled)\n",
    "split_point = int(len(repo_data_with_index_documents) * 0.9)\n",
    "repo_data_train = shuffled[:split_point]\n",
    "repo_data_test = shuffled[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "52ab8f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65dd2a2",
   "metadata": {},
   "source": [
    "## Adathalmaz átalakítása a fine tuning-hoz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "616b0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DISTRACTORS = 4 #Distractor dokumentumok száma\n",
    "PERCENT_D_STAR_INCLUDED = 0.8 #D* arány"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bda06d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt\n",
    "\n",
    "llm_mistral = ChatMistralAI(model_name=\"mistral-small\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4449edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A D* megtalálásához szükséges segéd függvény\n",
    "#Egy llm-et kérdez meg, hogy a kérdés válasz alapján melyik dokumentumban van a válasz\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm_mistral = ChatMistralAI(model_name=\"mistral-small\", temperature=0)\n",
    "\n",
    "def call_llm_to_find_doc(question, answer, candidate_docs):\n",
    "    prompt = f\"\"\"You are an AI assistant helping with document understanding.\n",
    "\n",
    "Question: \"{question}\"\n",
    "Answer: \"{answer}\"\n",
    "\n",
    "Here are some candidate document chunks (labeled D1, D2, ..., Dn):\n",
    "\n",
    "\"\"\"\n",
    "    for i, doc in enumerate(candidate_docs):\n",
    "        prompt += f\"D{i+1}: {doc}\\n\\n\"\n",
    "\n",
    "    prompt += \"\"\"\n",
    "Please analyze the documents above and decide which document (D1, D2, ..., Dn) most likely contains the answer to the question. \n",
    "\n",
    "If none of the documents are relevant or contain the answer, reply with \"None\".\n",
    "Otherwise, reply with the label of the best matching document, such as \"D2\".\n",
    "Only respond with \"None\" or a label like \"D3\"—no explanation.\n",
    "\"\"\"\n",
    "\n",
    "    response = llm_gemini.invoke([HumanMessage(content=prompt)])\n",
    "    reply = response.content.strip()\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "60b1bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A D* megtalálásához szükséges fő függvény\n",
    "#Megkeresi a dokumentumot a kérdés-válasz alapján, amiben a válasz található\n",
    "def find_D_star_document(question, answer, all_docs, embeddings, index):\n",
    "\n",
    "    \n",
    "    candidate_docs = retrieve_relevant_document(question, index, all_docs, embeddings, k=10)\n",
    "\n",
    "    if not candidate_docs:\n",
    "        print(\"Nem találtunk releváns dokumentumot.\")\n",
    "        return None\n",
    "\n",
    "    best_doc_label = call_llm_to_find_doc(question, answer, candidate_docs)\n",
    "\n",
    "    best_doc_label = best_doc_label.strip('\"')\n",
    "    if best_doc_label.lower() == \"none\":\n",
    "        return None\n",
    "\n",
    "    if best_doc_label.startswith(\"D\") and best_doc_label[1:].isdigit():\n",
    "        doc_index = int(best_doc_label[1:]) - 1\n",
    "        if 0 <= doc_index < len(candidate_docs):\n",
    "            return candidate_docs[doc_index]\n",
    "\n",
    "    print(\"A LLM válasza nem értelmezhető:\", best_doc_label)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ddafb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_distractor_documents(question, D_star_doc, all_docs, embeddings, index, k):\n",
    "    retrieved_docs = retrieve_relevant_document(question, index, all_docs, embeddings, k=k*2 + 1) #Több választ kér, hogy legyen benne Di is\n",
    "\n",
    "    distractors = []\n",
    "    for doc in retrieved_docs:\n",
    "        if doc != D_star_doc and doc not in distractors: #Ellenőrzi, hogy ne legyen benne a D* dokumentum\n",
    "            distractors.append(doc)\n",
    "        if len(distractors) == k:\n",
    "            break\n",
    "\n",
    "    #Ha nincs még elég distractor, akkor random választ a többi dokumentumból\n",
    "    attempts = 0\n",
    "    max_attempts = k * 5\n",
    "    while len(distractors) < k and attempts < max_attempts:\n",
    "        random_doc = random.choice(all_docs)\n",
    "        if random_doc != D_star_doc and random_doc not in distractors:\n",
    "            distractors.append(random_doc)\n",
    "        attempts += 1\n",
    "\n",
    "    return distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e894effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chain-of-Thought (CoT) generálás\n",
    "def generate_cot_answer(question, D_star_doc, original_answer):\n",
    "    prompt = f\"\"\"Given the Question, the Context (containing the golden document with the answer), and the original concise Answer, provide a detailed reasoning process (Chain-of-Thought) that explains step-by-step how to arrive at the Answer using *only* the provided Context.\n",
    "\n",
    "Crucially, you MUST cite the exact sentences or phrases from the Context that support your reasoning. Enclose these citations within ##begin_quote## and ##end_quote## tags. Do not add any information not present in the Context.\n",
    "\n",
    "Finally, state the concise Answer clearly. Format your response *exactly* as:\n",
    "##Reason: [Your detailed reasoning with citations like ##begin_quote## text from context ##end_quote##.]\n",
    "##Answer: [The final concise answer, matching the original answer provided]\n",
    "\n",
    "Question: {question}\n",
    "Context: {D_star_doc}\n",
    "Original Answer: {original_answer}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm_mistral.invoke([HumanMessage(content=prompt)])\n",
    "    reply = response.content.strip()\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAFT-hoz megfelelő adatok generálása\n",
    "raft_training_data = []\n",
    "skipped_count = 0\n",
    "counter = 0\n",
    "\n",
    "for item in repo_data_train:\n",
    "    question = item['question']\n",
    "    original_answer = item['answer']\n",
    "    index = item['index']\n",
    "    all_document_chunks = item['documents']\n",
    "\n",
    "    #Először meg kell találni a D* dokumentumot\n",
    "    D_star_doc = find_D_star_document(question, original_answer, all_document_chunks, OpenAIEmbeddings(), index)\n",
    "    if not D_star_doc:\n",
    "        print(f\"A '{question} ' kérdés kihagyása. D* dokumentum nem található.\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    #CoT válasz generálása\n",
    "    cot_answer_str = generate_cot_answer(question, D_star_doc, original_answer)\n",
    "    if not cot_answer_str:\n",
    "        print(f\"A '{question} ' kérdés kihagyása.  CoT generálás sikertelen.\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    #Distractor dokumentumok kiválasztása\n",
    "    distractor_docs = select_distractor_documents(question, D_star_doc, all_document_chunks, OpenAIEmbeddings(), index, k=NUM_DISTRACTORS)\n",
    "\n",
    "    #A D* benne legyen-e a válaszban\n",
    "    include_D_star = random.random() < PERCENT_D_STAR_INCLUDED\n",
    "\n",
    "    context_docs_for_instance = []\n",
    "    if include_D_star:\n",
    "        #D* dokumentum benne van a válaszban\n",
    "        context_docs_for_instance.append(D_star_doc)\n",
    "        context_docs_for_instance.extend(distractor_docs)\n",
    "    else:\n",
    "        #Csak a distractor dokumentumokat tartalmazza\n",
    "        context_docs_for_instance.extend(distractor_docs)\n",
    "\n",
    "\n",
    "    random.shuffle(context_docs_for_instance) #a dokumentumok megkeverése\n",
    "\n",
    "    context_string = \"\\n\\n\".join(context_docs_for_instance) #A kontexus létrehozása\n",
    "\n",
    "    #fine-tuning-hez megfelelő formátum létrehozása \n",
    "    formatted_input = f\"Question: {question}\\n\\nContext:\\n{context_string}\"\n",
    "    formatted_output = cot_answer_str #CoT válasz\n",
    "\n",
    "    raft_training_data.append({\n",
    "        \"input\": formatted_input,\n",
    "        \"output\": formatted_output\n",
    "    })\n",
    "\n",
    "    print(f\"\\nGenerated data for question: {question}, processed {counter} questions.\")\n",
    "    time.sleep(10)\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "41e18a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(raft_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7565bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raft_training_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(raft_training_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "00ae7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raft_training_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raft_training_data_loaded = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "raft_training_data_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca7ebf",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "bba6dcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konvertálva 170\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "openai_formatted_filename = \"openai_formatted_training_data.jsonl\"\n",
    "base_model = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "\n",
    "#Konverzió OpenAI chat formátumra\n",
    "\n",
    "converted_count = 0\n",
    "with open(openai_formatted_filename, 'w', encoding='utf-8') as outfile:\n",
    "    for raft_example in raft_training_data_loaded:\n",
    "        \n",
    "        user_content = raft_example.get(\"input\")\n",
    "        assistant_content = raft_example.get(\"output\")\n",
    "\n",
    "        if not user_content or not assistant_content:\n",
    "            print(f\" Az 'input' vagy 'output' üres: {raft_example}\")\n",
    "            continue\n",
    "\n",
    "        openai_message = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        outfile.write(json.dumps(openai_message) + '\\n')\n",
    "        converted_count += 1\n",
    "\n",
    "print(f\"Konvertálva {converted_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f093f74",
   "metadata": {},
   "source": [
    "### Openai fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2a626983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fájl feltöltése fine-tuninghoz\n",
    "\n",
    "with open(openai_formatted_filename, \"rb\") as f:\n",
    "    training_file = client.files.create(\n",
    "        file=f,\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "training_file_id = training_file.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "4659b2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-XEMqQQJDh3zWmGYsewtHAv'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "bc17ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-Tuning Job\n",
    "\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    model=base_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "7828f548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 12:49:56] Job Status: succeeded\n"
     ]
    }
   ],
   "source": [
    "#A fine tuning folyamatának figyelése\n",
    "job_id = job.id\n",
    "job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "status = job_status.status\n",
    "print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Job Status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model_id = job_status.fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "99caa5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model_id_openai= \"ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240fbbe",
   "metadata": {},
   "source": [
    "### Mistral fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9e779835",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model_id_mistral = \"ft:mistral-large-latest:5a036207:20250505:bcb27770\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910df8bb",
   "metadata": {},
   "source": [
    "## Fine-tuning + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "826717ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_raft = ChatMistralAI(model_name=fine_tuned_model_id_mistral, temperature=0)\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_raft = prompt | llm_raft\n",
    "\n",
    "chain_og = prompt | llm_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "1ae263b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Kérdés: Why does the `fs::path` wrapper disallow using `std::string` for path construction and conversion?\n",
      "The `fs::path` wrapper disallows using `std::string` for path construction and conversion to avoid locale-dependent decoding and encoding on Windows, which can lead to unsafe and unpredictable behavior.\n"
     ]
    }
   ],
   "source": [
    "#Tesztelés egy kérdésen\n",
    "\n",
    "sample_query = repo_data_test[0]['question']\n",
    "print(f\" Kérdés: {sample_query}\")\n",
    "\n",
    "index = repo_data_test[0]['index']\n",
    "documents = repo_data_test[0]['documents']\n",
    "\n",
    "sample_relevant_docs = retrieve_relevant_document(sample_query, index, documents, OpenAIEmbeddings())\n",
    "sample_context_str = \"\\n\\n\".join(sample_relevant_docs)\n",
    "\n",
    "response_raft = chain_raft.invoke({\n",
    "    \"context\": sample_context_str,\n",
    "    \"question\": sample_query\n",
    "})\n",
    "\n",
    "\n",
    "print(response_raft.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9743d15",
   "metadata": {},
   "source": [
    "## RAG Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042355e",
   "metadata": {},
   "source": [
    "4 kérdést gyárt a megadott kérdésből, azokból kapott válaszokból generálja a végső választ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb9795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3fd2c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatMistralAI(model_name=\"mistral-small\", temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "c2f14f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "#eredmények rankolása\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    \n",
    "    time.sleep(5)\n",
    "    return reranked_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00c41b",
   "metadata": {},
   "source": [
    "## HyDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec60bf5",
   "metadata": {},
   "source": [
    "A nyelvi modell először generál egy lehetséges válaszdokumentumot a kérdés alapján. Ezt a feltételezett válaszdokumentumot átdolgozza embeddinggé.\n",
    "\n",
    "Ezzel az embeddinggel keres dokumentumokat egy tudásbázisban.\n",
    "\n",
    "A lekért dokumentumokból a modell végül valódi választ generál."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a49633dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatMistralAI(temperature=0) | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb4431",
   "metadata": {},
   "source": [
    "## Kiértékelés RAGAs-sal a teszthalmazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "ad011fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def evaluate_test_data(llm, retriever_type = \"embedding\", rag_fusion = False, hyde = False):\n",
    "    questions = [item[\"question\"] for item in repo_data_test]\n",
    "    ground_truths = [item[\"answer\"] for item in repo_data_test]\n",
    "    \n",
    "    answers = []\n",
    "    contexts = []\n",
    "\n",
    "    for query in questions:\n",
    "\n",
    "        index = [item[\"index\"] for item in repo_data_test if item[\"question\"] == query][0]\n",
    "        documents = [item[\"documents\"] for item in repo_data_test if item[\"question\"] == query][0]\n",
    "        \n",
    "        #context megszerzése a választott retriever-től függően\n",
    "        context =[]\n",
    "        if retriever_type == \"bm25\":\n",
    "            bm25, tokenized_docs = build_bm25_index(documents)\n",
    "            context = retrieve_relevant_document_bm25(query, bm25, documents, tokenized_docs)\n",
    "\n",
    "        elif retriever_type == \"hybrid\":\n",
    "            bm25, tokenized_docs = build_bm25_index(documents)\n",
    "            context = retrieve_relevant_document_hybrid(query, bm25, tokenized_docs, index, OpenAIEmbeddings(), documents)\n",
    "        else:\n",
    "            context = retrieve_relevant_document(query, index, documents, OpenAIEmbeddings(), 10)\n",
    "\n",
    "        #Ha a HYDE-t használjuk\n",
    "        if hyde:\n",
    "            retriever = RunnableLambda(lambda query: retrieve_relevant_document(query, index, documents, OpenAIEmbeddings(), 10))\n",
    "            retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "            retrieved = retrieval_chain.invoke({\"question\":query})\n",
    "            context = retrieved\n",
    "\n",
    "        template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "        #Ha a RAG-fusion-t használjuk\n",
    "        if rag_fusion:\n",
    "\n",
    "            retriever = RunnableLambda(lambda query: retrieve_relevant_document(query, index, documents, OpenAIEmbeddings(), 10))\n",
    "\n",
    "            retrieval_chain_rag_fusion = (\n",
    "                generate_queries\n",
    "                | retriever.map()\n",
    "                | reciprocal_rank_fusion\n",
    "            )\n",
    "\n",
    "            final_rag_chain = (\n",
    "                {\"context\": retrieval_chain_rag_fusion, \n",
    "                \"question\": itemgetter(\"question\")} \n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "        \n",
    "            answers.append(final_rag_chain.invoke({\"question\":query}))\n",
    "        else:\n",
    "            chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "            content = chain.invoke({\"context\": context,\"question\":query})\n",
    "            answers.append(content)\n",
    "                \n",
    "        \n",
    "        contexts.append(contexts)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"reference\": ground_truths\n",
    "    }\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "        ],\n",
    "        \n",
    "    )\n",
    "\n",
    "    df = result.to_pandas()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "4345b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_evaluation_results(llm, retriever_type = \"embedding\", rag_fusion = False, hyde = False):\n",
    "    df = evaluate_test_data(llm, retriever_type, rag_fusion, hyde)\n",
    "\n",
    "    context_prediction = df[\"context_precision\"].mean()\n",
    "    context_recall = df[\"context_recall\"].mean()\n",
    "    faithfulness = df[\"faithfulness\"].mean()\n",
    "    answer_relevancy = df[\"answer_relevancy\"].mean()\n",
    "\n",
    "    print(f\"Context Precision: {context_prediction:.4f}\")\n",
    "    print(f\"Context Recall: {context_recall:.4f}\")\n",
    "    print(f\"Faithfulness: {faithfulness:.4f}\")\n",
    "    print(f\"Answer Relevancy: {answer_relevancy:.4f}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_evaluation_results(llm, retriever_type = \"embedding\", rag_fusion = False, hyde = False):\n",
    "    df = evaluate_test_data(llm, retriever_type, rag_fusion, hyde)\n",
    "\n",
    "    context_prediction = df[\"context_precision\"].mean()\n",
    "    context_recall = df[\"context_recall\"].mean()\n",
    "    faithfulness = df[\"faithfulness\"].mean()\n",
    "    answer_relevancy = df[\"answer_relevancy\"].mean()\n",
    "\n",
    "    return context_prediction, context_recall, faithfulness, answer_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "llm_values = [llm_mistral, llm_raft_mistral, llm_raft_openai]\n",
    "retriever_types = [\"embedding\", \"bm25\", \"hybrid\"]\n",
    "rag_fusion_options = [True, False]\n",
    "hyde_options = [True, False]\n",
    "\n",
    "\n",
    "combinations = list(product(llm_values, retriever_types, rag_fusion_options, hyde_options))\n",
    "\n",
    "results = []\n",
    "for llm, retriever, fusion, hyde in combinations:\n",
    "    ctx_pred, ctx_recall, faithful, answer_rel = calc_evaluation_results(llm, retriever, fusion, hyde)\n",
    "    results.append({\n",
    "        \"LLM\": llm,\n",
    "        \"Retriever\": retriever,\n",
    "        \"RAG Fusion\": fusion,\n",
    "        \"HyDE\": hyde,\n",
    "        \"Context Prediction\": ctx_pred,\n",
    "        \"Context Recall\": ctx_recall,\n",
    "        \"Faithfulness\": faithful,\n",
    "        \"Answer Relevancy\": answer_rel\n",
    "    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bcf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ae4e5ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLM</th>\n",
       "      <th>Retriever</th>\n",
       "      <th>RAG Fusion</th>\n",
       "      <th>HyDE</th>\n",
       "      <th>Context Prediction</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>embedding</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.615397</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.574074</td>\n",
       "      <td>0.776850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>embedding</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.639254</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.665139</td>\n",
       "      <td>0.766164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>embedding</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.576892</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.867447</td>\n",
       "      <td>0.715350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>embedding</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.641043</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.737698</td>\n",
       "      <td>0.773911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>bm25</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.569474</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.544032</td>\n",
       "      <td>0.718299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>bm25</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.361042</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.260615</td>\n",
       "      <td>0.762268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>bm25</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.617678</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.773844</td>\n",
       "      <td>0.810178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>bm25</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.361042</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.656508</td>\n",
       "      <td>0.567507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.578968</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.587954</td>\n",
       "      <td>0.764314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.464167</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.352163</td>\n",
       "      <td>0.672569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.616357</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.761960</td>\n",
       "      <td>0.765787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000017FB38F3040&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018013516AD0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.405833</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.668631</td>\n",
       "      <td>0.626751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>embedding</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.587525</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.683249</td>\n",
       "      <td>0.722348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>embedding</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.616570</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.772064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>embedding</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.603731</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.845536</td>\n",
       "      <td>0.811334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>embedding</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.639709</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.883382</td>\n",
       "      <td>0.772028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>bm25</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.570970</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.672500</td>\n",
       "      <td>0.772991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>bm25</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.311042</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.347143</td>\n",
       "      <td>0.770453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>bm25</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.611055</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.936923</td>\n",
       "      <td>0.723003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>bm25</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.311042</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.427643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.561107</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.642667</td>\n",
       "      <td>0.771316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.466944</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.379608</td>\n",
       "      <td>0.765966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.576872</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.852648</td>\n",
       "      <td>0.814313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>client=&lt;httpx.Client object at 0x0000018019E81930&gt; async_client=&lt;httpx.AsyncClient object at 0x0000018019E83AF0&gt; mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.468333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.572517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>embedding</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.583278</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.587956</td>\n",
       "      <td>0.821499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>embedding</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.607105</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.731548</td>\n",
       "      <td>0.869356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>embedding</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.606077</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.831067</td>\n",
       "      <td>0.813894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>embedding</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.637849</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.780307</td>\n",
       "      <td>0.812944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>bm25</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.597059</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.658214</td>\n",
       "      <td>0.816754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>bm25</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.311042</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.293988</td>\n",
       "      <td>0.765307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>bm25</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.573741</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.813492</td>\n",
       "      <td>0.721549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>bm25</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.701972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.645635</td>\n",
       "      <td>0.880151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.362745</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.861151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.782931</td>\n",
       "      <td>0.735282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>client=&lt;openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80&gt; async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930&gt; root_client=&lt;openai.OpenAI object at 0x000001801A8601C0&gt; root_async_client=&lt;openai.AsyncOpenAI object at 0x000001801AFD3BB0&gt; model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.676953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                   LLM  \\\n",
       "0                                                                                                                                                                                                                                        client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "1                                                                                                                                                                                                                                        client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "2                                                                                                                                                                                                                                        client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "3                                                                                                                                                                                                                                        client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "4                                                                                                                                                                                                                                        client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "5                                                                                                                                                                                                                                        client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "6                                                                                                                                                                                                                                        client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "7                                                                                                                                                                                                                                        client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "8                                                                                                                                                                                                                                        client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "9                                                                                                                                                                                                                                        client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "10                                                                                                                                                                                                                                       client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "11                                                                                                                                                                                                                                       client=<httpx.Client object at 0x0000017FB38F3040> async_client=<httpx.AsyncClient object at 0x0000018013516AD0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' temperature=0.0 model_kwargs={}   \n",
       "12                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "13                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "14                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "15                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "16                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "17                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "18                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "19                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "20                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "21                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "22                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "23                                                                                                                                                                            client=<httpx.Client object at 0x0000018019E81930> async_client=<httpx.AsyncClient object at 0x0000018019E83AF0> mistral_api_key=SecretStr('**********') endpoint='https://api.mistral.ai/v1' model='ft:mistral-large-latest:5a036207:20250504:af2c1927' temperature=0.0 model_kwargs={}   \n",
       "24  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "25  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "26  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "27  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "28  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "29  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "30  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "31  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "32  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "33  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "34  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "35  client=<openai.resources.chat.completions.completions.Completions object at 0x000001801AFD3B80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001801AFD9930> root_client=<openai.OpenAI object at 0x000001801A8601C0> root_async_client=<openai.AsyncOpenAI object at 0x000001801AFD3BB0> model_name='ft:gpt-3.5-turbo-1106:velkey::BTRAj6YH' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********')   \n",
       "\n",
       "    Retriever  RAG Fusion   HyDE  Context Prediction  Context Recall  \\\n",
       "0   embedding        True   True            0.615397        0.675000   \n",
       "1   embedding        True  False            0.639254        0.558333   \n",
       "2   embedding       False   True            0.576892        0.725000   \n",
       "3   embedding       False  False            0.641043        0.700000   \n",
       "4        bm25        True   True            0.569474        0.700000   \n",
       "5        bm25        True  False            0.361042        0.233333   \n",
       "6        bm25       False   True            0.617678        0.675000   \n",
       "7        bm25       False  False            0.361042        0.233333   \n",
       "8      hybrid        True   True            0.578968        0.725000   \n",
       "9      hybrid        True  False            0.464167        0.383333   \n",
       "10     hybrid       False   True            0.616357        0.675000   \n",
       "11     hybrid       False  False            0.405833        0.325000   \n",
       "12  embedding        True   True            0.587525        0.675000   \n",
       "13  embedding        True  False            0.616570        0.550000   \n",
       "14  embedding       False   True            0.603731        0.650000   \n",
       "15  embedding       False  False            0.639709        0.550000   \n",
       "16       bm25        True   True            0.570970        0.750000   \n",
       "17       bm25        True  False            0.311042        0.233333   \n",
       "18       bm25       False   True            0.611055        0.683333   \n",
       "19       bm25       False  False            0.311042        0.233333   \n",
       "20     hybrid        True   True            0.561107        0.633333   \n",
       "21     hybrid        True  False            0.466944        0.383333   \n",
       "22     hybrid       False   True            0.576872        0.683333   \n",
       "23     hybrid       False  False            0.468333        0.333333   \n",
       "24  embedding        True   True            0.583278        0.625000   \n",
       "25  embedding        True  False            0.607105        0.633333   \n",
       "26  embedding       False   True            0.606077        0.650000   \n",
       "27  embedding       False  False            0.637849        0.650000   \n",
       "28       bm25        True   True            0.597059        0.608333   \n",
       "29       bm25        True  False            0.311042        0.233333   \n",
       "30       bm25       False   True            0.573741        0.650000   \n",
       "31       bm25       False  False                 NaN        0.211111   \n",
       "32     hybrid        True   True                 NaN        0.677778   \n",
       "33     hybrid        True  False            0.466667        0.362745   \n",
       "34     hybrid       False   True                 NaN        0.647059   \n",
       "35     hybrid       False  False                 NaN        0.468750   \n",
       "\n",
       "    Faithfulness  Answer Relevancy  \n",
       "0       0.574074          0.776850  \n",
       "1       0.665139          0.766164  \n",
       "2       0.867447          0.715350  \n",
       "3       0.737698          0.773911  \n",
       "4       0.544032          0.718299  \n",
       "5       0.260615          0.762268  \n",
       "6       0.773844          0.810178  \n",
       "7       0.656508          0.567507  \n",
       "8       0.587954          0.764314  \n",
       "9       0.352163          0.672569  \n",
       "10      0.761960          0.765787  \n",
       "11      0.668631          0.626751  \n",
       "12      0.683249          0.722348  \n",
       "13      0.682353          0.772064  \n",
       "14      0.845536          0.811334  \n",
       "15      0.883382          0.772028  \n",
       "16      0.672500          0.772991  \n",
       "17      0.347143          0.770453  \n",
       "18      0.936923          0.723003  \n",
       "19      0.937500          0.427643  \n",
       "20      0.642667          0.771316  \n",
       "21      0.379608          0.765966  \n",
       "22      0.852648          0.814313  \n",
       "23      0.845000          0.572517  \n",
       "24      0.587956          0.821499  \n",
       "25      0.731548          0.869356  \n",
       "26      0.831067          0.813894  \n",
       "27      0.780307          0.812944  \n",
       "28      0.658214          0.816754  \n",
       "29      0.293988          0.765307  \n",
       "30      0.813492          0.721549  \n",
       "31      0.604167          0.701972  \n",
       "32      0.645635          0.880151  \n",
       "33      0.333333          0.861151  \n",
       "34      0.782931          0.735282  \n",
       "35      0.616667          0.676953  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
